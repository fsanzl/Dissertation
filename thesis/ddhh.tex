\chapter{Humanidades digitales}
\epigraphhead[50]{\begin{english}\epigraphtextposition{flushleftright}
	\epigraph{Ces gens là serviront d'exemple terrible à tous ceux qui prennent le monde pour un théâtre où la force technique et le triomphe de cette force mènent librement leur jeu.}{Jean-Luc Godard, \textit{Alphaville, une étrange aventure de Lemmy Caution}}
\end{english}
}
\section{¿Qué son las humanidades digitales?}\index{humanidades digitales}
En esta primera parte, introduciremos los elementos teóricos de los que habremos de valernos en la segunda. Como es menester, hay que contextualizar la tarea que nos ocupa en el marco metodológico en el que se ubica. La travesía que en este puerto iniciamos transcurre por las aguas de las \textit{humanidades digitales}\index{humanidades digitales}. Es de recibo, pues, perfilar ese ámbito de actuación para navegarlo con garantías y el primer reto que se plantea es dilucidar la naturaleza epistemológica y ontológica de la disciplina. Después de todo, sus lindes constreñirán el trabajo y, precisamente por eso, debemos apurarlos cuanto sea posible.

La labor no se antoja sencilla. Las humanidades digitales son un campo en continuo y rápido desarrollo, una disciplina que se expande abrazando nuevos objetos de estudio a la vez que produce otros a resultas de los trabajos en curso. Por si fuera poco, los avances técnicos y metodológicos de los últimos años han acelerado el paso. A causa de esto, se complica proponer una definición válida y, con más razón si cabe, una que lo siga siendo ante los embates del tiempo en un entorno siempre cambiante. ¿Cómo abordar un concepto que abarca elementos tan diferentes a primera vista como los análisis cuantitativos asistidos por computador, la digitalización de documentos y, de la conjugación de ambos, los archivos digitales, visualizaciones de datos, sistemas de información geográfica, materiales multimedia y un sinfín de aplicaciones más? 

Intentaremos acercarnos a la respuesta a lo largo de las siguientes páginas en la medida de lo posible, si bien con la prevención de que lo haremos desde una perspectiva filológica, por ser esta la que nos concierne. No obstante, rogamos al lector que no pierda de vista que cuanto exponemos es, por lo general, extrapolable a \textit{textos} en el sentido más amplio, tal y como entiende el término la semiótica, allende lo puramente lingüístico y, por lo tanto, aplicable a otras disciplinas humanísticas.

Quizás podríamos evitarnos problemas si atajamos la discusión aquí mismo confiando la respuesta a la fe y la esperanza, aceptando caritativamente la primera propuesta de \citeauthor{scholes2008}: «\begin{english}If ``\textit{fiat lux}" worked for the designer of this universe, with light appearing in answer to its name, perhaps ``\textit{fiat studies digitales}" will work for this field. The magical power of names should be respected. \textit{En arche en ho logos} indeed\end{english}» \parencite[50]{scholes2008}. ¿Sería lícito afirmar que las humanidades digitales han estado latentes en un limbo ontológico a la espera de ser nombradas para empezar a existir?

Contra lo que pueda parecer a primera vista, la argumentación de Scholes y Wulfman no deja de tener cierto poso de verdad. Los estudiosos empezaron a valerse de lo que hoy se conoce por humanidades digitales como herramienta y no como método. Los primeros experimentos apenas suponían una diferencia cuantitativa con respecto a los estudios más tradicionales; conceptualmente, la velocidad en que se manejan registros manual y mecánicamente es un detalle irrelevante, con independencia de que, a efectos prácticos, suponga un cambio descomunal. Valga la metáfora, los corpus textuales eran una suerte de \textit{Mare Nostrum} que podía navegarse con una galera o una nao para llegar a un mismo puerto, sin más distinción que el esfuerzo humano requerido por una y otra embarcación y la carga transportada. El punto de inflexión lo marca el aprovechamiento de la capacidad del barco para lanzarse a mar abierto en una larga travesía, atravesar el océano incluso. Bastaría la voz de un almirante ordenando traspasar las Columnas de Hércules y singlar hacia el poniente. Y, aun así, aunque el audaz marinero triunfara en su empresa, no tendría una ruta de las Indias, sino una osada aventura, al menos hasta que diera noticias del Nuevo Mundo a su regreso. Si no se conoce el destino, no existe ruta. Como cantó el poeta sevillano, «se hace camino al andar».

De la misma manera, podría decirse que los pioneros de la \textit{humanities computing}\index{humanities computing@{\textit{humanities computing}}}\footnote{Nos quedaremos con el anglicismo para la etapa histórica de la disciplina y usaremos \textit{computación aplicada a las humanidades} como concepto general.} tampoco estaban abriendo una nueva vía, sino adaptando las herramientas modernas, ahora a su alcance, a necesidades sempiternas, usaban naves más grandes para transportar la misma mercancía a los mismos destinos, tal y como lo habían hecho sus predecesores hasta aquel momento, si bien de forma más eficiente. Sin embargo, los protohumanistas digitales más visionarios se percataron de que esa mejorada capacidad podía emplearse de otro modo, al igual que, en la navegación, el volumen de las modernas embarcaciones fue decisivo para alcanzar  sin escalas costas más lejanas. Era cuestión de tiempo que alguien intentara ponerlo en práctica para llegar más allá, hasta donde nadie lo había hecho antes. Como con los navegantes de antaño, según se iban transgrediendo los límites conocidos por la computación aplicada a las humanidades, se difuminaba también la naturaleza exclusivamente instrumental del computador y, además, con ello, la validez descriptiva de aquel nombre primigenio.

Dejando a un lado digresiones navales y metafísicas, lo cierto es que las definiciones más estrictas excluyen aspectos hoy asociados a las humanidades digitales, mientras que las más laxas tienden a serlo tanto que comprenderían cualquier cosa remotamente relacionada con lo digital o con las humanidades. ¿Es el candidato lo suficientemente humanístico, lo suficientemente digital? ¿Nos limitamos a la disciplina académica o incluimos su objeto de estudio? Podemos estudiar medios tradicionales digitalmente, pero también hacerlo tradicionalmente con medios digitales. Lo mismo que se aplican métodos computacionales al estudio de la literatura, puede estudiarse un videojuego desde una perspectiva estrictamente narratológica. Si hablamos de \textit{humanities computing}\index{humanities computing@{\textit{humanities computing}}}, el límite parece claro, pero el término \textit{humanidades digitales} da pie a las más diversas interpretaciones, algunas incluso mutuamente excluyentes.

Por lo tanto, resulta más sensato acercarse lentamente al concepto que nos ocupa antes de aventurar una definición, dando vueltas en espiral en torno a él para aproximarse en cada ciclo un poco más al punto central, a la esencia. Empezaremos por establecer qué hacen las humanidades digitales, lo que, según \citeauthor{burdick2012}~\parencite*[17-20]{burdick2012}, es ocuparse fundamentalmente del \textit{curación}, \textit{análisis}, \textit{edición} y \textit{modelado} de datos. Con curación, se refieren a aquellas actividades dedicadas a la selección y organización de materiales, como podrían ser los archivos o las bibliotecas digitales. En el ámbito de las humanidades digitales, esto también implica un elemento interpretativo, pues aprovecha los recursos digitales para mejorar la transmisión del material y contextualizarlo. Dicho de otro modo, no tratar el objeto como un elemento autónomo, sino dentro de la red que constituye junto con otros objetos y con sus respectivos metadatos.  

Añadimos asimismo una definición finalista, señalando su objeto. Las humanidades digitales se ocupan de textos —entiéndase, insistimos, en el sentido semiótico y no lingüístico, porque ahora no nos referimos específicamente a la filología, sino a las humanidades en general—, y estos textos no se limitan a los tradicionales, pues pueden adoptar nuevas formas gracias a la técnica. Nos referimos también al empleo de la tecnología digital para facilitar el acceso a información humanística tradicional, como hacen las ediciones digitales y las colecciones de documentos digitalizados, pero también al empleo archivístico para almacenar y catalogar materiales de un modo que sería imposible de no existir esta tecnología. Hay quien sostiene incluso que los actos de creación artística asistidos por un computador forman parte de las humanidades digitales \parencite[52-55]{scholes2008}.

No tendríamos problema en seguir añadiendo descripciones desde diferentes perspectivas, por ejemplo, mediante la enumeración de algunas propiedades singulares que poseen.  Así, \citeauthor{unsworth2002}~\parencite*{unsworth2002} sostiene que, por un lado, se trata de un modelo mimético del objeto que pretende estudiar, pero también una manera de razonamiento, así como un conjunto de compromisos ontológicos. Además, todo lo anterior está determinado por el equilibrio entre la necesidad de una computación eficiente y la comunicación humana.

De la misma manera, \citeauthor{drucker2021}~\parencite*[2]{drucker2021} llama la atención sobre las diferencias en los materiales, su procesamiento y la presentación de los resultados obtenidos como rasgos propios de las humanidades digitales. Recalca la importancia de los materiales porque adaptar las fuentes a un formato digital requiere una mediación que tiene implicaciones en el modo en que estos podrán usarse. Esto, precisamente, debemos considerarlo con extremo cuidado a la hora de elaborar la parte empírica de nuestro trabajo, pues la forma en que se dispongan los resultados determinará en gran medida el ámbito de las investigaciones capaz de valerse de ellos. 

En cuanto a la función mimética, partiremos de la premisa de que podemos crear representaciones de un objeto, pero serán siempre incompletas. La mímesis ha de ser necesariamente una simplificación porque solo el propio objeto se representa a sí mismo con total precisión. Incluso si fuera posible hacer una copia exacta a nivel atómico de un objeto dado, la copia seguiría careciendo de propiedades esenciales del original, como su ubicación en el espacio y en el tiempo. Para las obras de arte, hablaríamos del \textit{aura} de la pieza original \parencite{benjamin2013}. A pesar de esta incompletitud, los modelos no dejan de ser una herramienta preciosa para el desarrollo de los humanos, que se han servido de ellos desde sus orígenes, milenios antes de que alguien pudiera siquiera soñar con la computación. Nuestro conocimiento de todos los aspectos del mundo se basa en modelos parciales que vamos desbastando, puliendo y perfeccionando a lo largo del tiempo.

Supongamos, por ejemplo, una lista de frecuencias de las palabras de una obra literaria. Tal lista no es el texto, pero se refiere a él en cierta manera y lo describe parcialmente. No se trata de una descripción completa —que solo el propio texto podría ofrecer—, sino que está sesgada: muestra únicamente la parte que hemos decidido representar. Conocemos las palabras y su frecuencia; sin embargo, ignoramos dónde aparecen y su posición en contexto respecto a las demás palabras.

Las diferentes formas de representación suponen modos de razonamiento derivados de ellas. \citeauthor{unsworth2002} apela a \citeauthor{minsky1977}~\parencite*{minsky1977}, quien sostiene que, ante una situación nueva, uno toma de su memoria un \textit{marco} — esto es, un escenario estereotípico— que recuerda y le es familiar, que adapta a la realidad mediante la alteración de detalles allí donde sea menester. Por lo tanto, si hemos de enfrentarnos a nuevas formas de representación, normalmente, construiremos un marco de referencia a partir de nuestra experiencia previa, y lo modificaremos en función de las características del nuevo sistema frente al que nos encontramos.

Volviendo a la lista de frecuencias, no resultaría muy descabellado aventurar una jerarquía de importancia según la lista. Podríamos incluso dejar solo las palabras de contenido léxico pleno y hacer inferencias semánticas al modo de una exégesis tradicional. Sin embargo, la representación jerárquica forzaría un proceso discursivo diferente al de la simple lectura. O, como lo formulan \citeauthor{burdick2012}~\parencite*{burdick2012}, la computación depende de la desambiguación en todas sus etapas, lo que contraviene la ambigüedad y presupuestos implícitos de las humanidades. El aspecto computacional demanda que los humanistas trabajen en los términos impuestos por las limitaciones tecnológicas. Paradójicamente, los trabajos humanísticos computacionales han modificado el proceso a su vez, por lo que se han ideado vías para introducir la ambigüedad y la contingencia en la informática.

Habíamos dicho que la representación y el modo de razonamiento obligan a aceptar un conjunto de compromisos ontológicos. Esto es, surgen nuevas entidades que han de tenerse en cuenta. \citeauthor{unsworth2002}~\parencite*{unsworth2002} pone como ejemplo una edición según las directrices de la Text Encoding Initiative (\ac{tei}) de la \textit{Divina Comedia} de Dante. En ella se marcan los cantos, estrofas y versos a la manera usual, pero también los nombres propios y epítetos, se distingue el origen de las fuentes entre históricas, mitológicas, bíblicas y literarias, los tipos de animal o persona, y otra metainformación. En el texto \ac{tei} se representan nuevas entidades, los nombres, y estos pertenecen a distintas categorías susceptibles de ser estudiadas. Los metadatos reflejan el texto, ofrecen un modelo cuyas entidades representan una parte de él.

¿Hasta qué punto debemos considerar \textit{nuevas}\footnote{Markus Ebenhoch, muy acertadamente, me hizo notar aquí la conveniencia de matizar que la novedad no es histórica, sino respecto al propio texto. Estos metadatos en poco se distinguen conceptualmente de las milenarias glosas. La diferencia estriba en el incremento astronómico de la capacidad para glosar el texto digitalmente que, como se verá en \Cref{sec:disread}, puede dar pie a enfoques muy diferentes a los clásicos.} esas entidades? En el ejemplo, estas existen en la edición \ac{tei}, aunque no en la representación tradicional. Cabría aducirse que las fuentes siempre han tenido el mismo origen, lo que no deja de ser correcto. No obstante, en el caso del texto por sí mismo, el modelo no es una parte intrínseca de él, sino que es una construcción externa. Un erudito familiarizado con las fuentes dantescas que leyera la \textit{Comedia} por primera vez podría ir clasificando todas estas mentalmente según avanza en la lectura. Sin embargo, un modelo así construido es extrínseco al texto, está solo en la mente del lector y, además, de forma incompleta, pues ha de irse construyendo necesariamente al vuelo, al paso que la vista progresa, verso a verso, terceto a terceto. El aparato crítico, por otra parte, no es mimético en circunstancias normales, pero tiene potencial para serlo: imaginemos, que describimos el texto en notas según los criterios de marcado mencionados en el párrafo precedente. La suma de todas las notas a pie de página ofrecería una idea precisa de algunos aspectos del texto, como el uso de las fuentes o el tipo de personajes que aparecen y en qué momento.  

Si vamos a poner a la máquina a estudiar las relaciones entre los nombres de los personajes de la \textit{Divina Comedia}, necesitaremos tener en cuenta el modo de operación del ingenio electrónico para plantearle cuestiones no solo computables, sino  que también puedan resolverse con eficiencia. Poco podemos hacer formulando un problema de complejidad exponencial cuya solución requiriera cientos de años\footnote{En la práctica, incluso mucho menos podría llegar a ser un margen intolerable si lo imponen los plazos.}. La otra cara de la moneda es la necesidad de servir a la comunicación humana. De escasa utilidad es que el problema sea computable si el resultado no ofrece información manejable ni sirve de consuelo que algo sea teóricamente posible si la complejidad de su codificación para que lo entienda la máquina escapa del alcance del encargado de la tarea.

En el prefacio de su influyente libro, \citeauthor{schreibman2004a} circunscriben los ámbitos potenciales de las humanidades digitales a una interpretación más estricta del término y limitan su espectro a la aplicación de métodos computacionales a las obras humanísticas y a los resultados de estos trabajos. Lo ponen de la siguiente manera:

\blockquote{\begin{english}In the digital humanities, critical inquiry involves the application of algorithmically facilitated search, retrieval, and critical processes that, originating in humanities-based work, have been demonstrated to have application far beyond. Associated with critical theory, this area is typified by interpretative studies that assist in our intellectual and aesthetic understanding of humanistic works. It also involves the application (and applicability) of critical and interpretative tools and analytic algorithms [...] on those artifacts produced through processes associated with archival representation made available via resources associated with processes of publishing and the communication of results.\end{english} \parencite[xxv]{schreibman2004a}}

Desde otra perspectiva, Laura~Hernández-Lorenzo~\parencite*{hernandez2022} reconoce la importancia de la interdisciplinariedad, que es obligada al tomar prestadas herramientas y métodos hasta ahora ajenos, trabajar con grandes cantidades de datos —posibilitado por lo anterior— y adoptar una sintonía con los avances tecnológicos actuales, que establece un nexo tangible con el desarrollo de la sociedad. Llama la atención también sobre otras características más notables, todas ellas con una vertiente ética y otra metodológica que se retroalimentan mutuamente. Estas incluyen la forma colaborativa de trabajo y su internacionalización, propiciados por la difusión y utilización de métodos y código abierto. Pasamos, pues, del estudioso solitario aislado en su despacho al equipo que, además, puede encontrarse repartido por todos los puntos del orbe\footnote{Conocido es el problema de la despoblación rural en España \parencite{esteban2022}. Cabría plantearse si las facilidades técnicas podrían ayudar a revertir esta tendencia demográfica del nuevo milenio.}.

Vemos así que, cuando se habla de humanidades digitales, nos hallamos ante un vasto campo en el que se entrecruzan y mezclan las letras y la computación, las humanidades y las ciencias exactas. Se centra en fuentes primarias tradicionales, pero también en nuevos objetos producidos digitalmente. Comprende el archivo de esos materiales, así como la democratización de su acceso\footnote{Esto, hasta cierto punto, ha cristalizado en los principios \acs{fair} \parencite{wilkinson2016} y su acogida por la comunidad científica.}. No se limita a la curación de los textos, sino que abraza su análisis digital. Su metodología parte de las humanidades, pero la aplica usando herramientas computacionales. Demanda estrecha colaboración  entre investigadores, aunque no depende de la coincidencia geográfica del equipo. De la combinación de todo esto, surgen nuevas ontologías, que suscitan a su vez nuevas preguntas, cuya respuesta exige nuevos métodos. Por lo tanto, debemos cuestionar estos métodos en primer lugar si albergamos la intención de aplicarlo con garantías.

\section{De \textit{computación de humanidades} a \textit{humanidades digitales}}
Las humanidades digitales\index{humanidades digitales}, incluso ya entrado el segundo milenio, se tienen por una aproximación novedosa en el ámbito de la filología. Sin embargo, aunque su aparición no se remonte a los inicios de la disciplina como rama de las ciencias humanas en los albores del siglo \textsc{xix} —si bien podrían encontrarse precedentes, como que Ada Lovelace, hija del poeta romántico Lord Byron, o que la matemática Anabella Milbanke, planteara emplear la pionera máquina de cálculo de Charles Babbage para fines distintos a las ciencias exactas \parencite[153]{hernandez2022}—, su historia va ya para los tres cuartos de siglo. Fueron asimismo los requerimientos de la investigación filológica los que dieron el impulso inicial a la \textit{humanities computing}\index{humanities computing@{\textit{humanities computing}}} y la fuerza motriz que mantuvo en marcha la nueva metodología durante sus vacilantes primeros pasos. Aún hoy, se siguen malinterpretando con frecuencia las humanidades digitales\index{humanidades digitales} como una rama de la lingüística aplicada, a pesar de que hace tiempo que forman parte de la caja de herramientas de aquellos que estudian campos tan diversos como las artes plásticas o la historia, la música o los medios de comunicación de masas. No entraremos más allá, puesto que es la investigación filológica la que nos motiva, pero emplazamos al lector curioso a completar lo aquí expuesto en otro lado \parencites{burdick2012}{berry2011b}{rio2014}{schreibman2004}\index{humanities computing@{\textit{humanities computing}}}.

 Para encontrar el germen de las humanidades digitales\index{humanidades digitales} debemos retroceder hasta la Italia de la década de 1940, donde, como cuenta \citeauthor{winter1999}~\parencite*{winter1999}, el joven Roberto Busa, hermano de la Compañía de Jesús, cavilaba sobre la metafísica de la \textit{presencia} en Santo Tomás. Alrededor de 1946, comenzó a escudriñar el corpus tomístico en busca de los términos \textit{praesens} y \textit{praesentia}, solo para concluir que eran periféricos y, si acaso, con cierto grado de conexión con la preposición \textit{in}. Busa iba anotando las ocurrencias en tarjetas de cartulina, de las que llegó a tener más de diez mil. Como es de suponer, mantener un índice manual de todas las palabras resultó ser al cabo del tiempo una tarea inasumible para el padre Busa y sus colaboradores.

El jesuita no cejó en su empeño. Bien al contrario, en 1949 se desplazó hasta los Estados Unidos para emprender un peregrinaje por las universidades de ese país en busca de una solución a su problema. Por una de esas jugadas de la fortuna, Busa llegó a la Universidad de Columbia precisamente cuando se hallaba allí Thomas J. Watson, cerebro de la empresa IBM a la sazón, que trataba de hacer presión en favor de la candidatura de Dwight D. Eisenhower para la presidencia de la institución académica. IBM ya había comenzado por aquel entonces a desarrollar los primeros computadores para fines comerciales; ese casual encuentro no pudo tener más felices consecuencias, ya que el tenaz jesuita italiano consiguió convencer a Watson para que pusiera a su disposición los medios que necesitaba.

Pese al escepticismo inicial de Watson en cuanto a la adecuación de las máquinas para la tarea, Busa creía en la idea de tomar herramientas desarrolladas originalmente para otros fines —como el comercio o los cálculos matemáticos en la investigación científica— y encontrarles otras aplicaciones. En la mente del italiano, esas «otras aplicaciones» significaban una sola cosa: un índice de concordancias de las obras completas de Santo Tomás de Aquino. De vuelta en Italia y ya con los medios técnicos a su disposición, el jesuita empezó por grabar el texto en tarjetas perforadas, que era el medio de almacenamiento de aquellas máquinas primitivas antes de que los soportes magnéticos tomaran el relevo en el panorama computacional. Una vez codificado el corpus de esta manera, escribió un programa para encontrar las concordancias. Habían nacido las humanidades digitales\index{humanidades digitales}.

Busa no se había equivocado. Aquel visionario consiguió aplicar a la filología herramientas que habían sido diseñadas con vista a los registros de datos que requiere el mundo empresarial y, de esta manera, fue el creador del primer generador automático de concordancias, que publicó en 1951: el \textit{Index Thomisticus} \parencite{busa1951}. Este índice trascendió su propio fin, porque, con él, se sentaron las bases de una nueva metodología de análisis textual y, por extensión, de aproximarse a las disciplinas humanísticas. Se trataba de lo que se dio en llamar \textit{humanities computing}\index{humanities computing@{\textit{humanities computing}}}.

Otras iniciativas reseñables para encontrar concordancias, en la misma línea que el proyecto inicial del padre Busa, fueron la del reverendo John W. Ellison con la versión estándar revisada de la \textit{Biblia} en inglés, que se completó en 1957. En aquella ocasión, no se emplearon máquinas de IBM, sino computadores Univac, y la de Parrish y Painter, que abordaba la literatura reciente con el poeta americano decimonónico Matthew Arnold y el irlandés W. B. Yeats \parencite[6-8]{burton1981}, fallecido apenas dos décadas antes.

Ese mismo año, encontró la muerte Guy Montgomery, sin haber completado su índice de palabras de John Dryden. Sus colaboradores continuaron el trabajo y, para ello, volvieron a recurrir a IBM. El resultado lo publicaron algunos años más tarde \parencite{montgomery1957}. Aquello permitió hacer apreciaciones cuantitativas respecto a la poesía de Dryden, como la recurrencia de los términos \textit{care}, \textit{fate}, \textit{friend}, \textit{head} y \textit{mind}. Con esto, el equipo de Montgomery estableció los fundamentos de los diccionarios de frecuencias de palabras y rimas que hoy continúan en servicio \parencite[4]{burton1981}.

También de la misma época datan los primeros esfuerzos en torno a la traducción automática. Andrew Booth y Richard Richens fueron pioneros en este campo, aunque la idea ya había surgido en la década precedente.
\blockquote{\begin{english}The suggestion that modem computing machines could be used for the purposes of translation originated with the present author. It arose because, in 1946, various new 	uses for automatic digital calculating machines were being considered and these ranged from the more obvious applications to problems of mathematics and physics, to philosophical problems such as the mechanization of human thought processes, the playing of games and the translation of language.\end{english} \parencite[88]{booth1958}}
El primer experimento se llevó a cabo en 1951 y consistía en traducir oraciones en ruso formadas con un cuarto de millar de palabras. La prueba obtuvo los resultados que Booth y Richens habían previsto, pero también confirmó sus sospechas en cuanto a las deficiencias que podían esperarse. En cualquier caso, se consideró lo suficientemente prometedor como para que pudieran seguir profundizando en la traducción por computador. La investigación siguió adelante y, algunos años después, publicaron todos los detalles del primer algoritmo de traducción automática \parencite{richens1955}. Booth recibió una posición en la Universidad de Londres a mediados de la década de 1950. Allí estaba surgiendo un cierto interés por aplicar los métodos digitales a la obra de Platón, en gran medida gracias al deseo de Leonard Brandwood de desarrollar una manera de encontrar el contexto de cualquier palabra en una edición determinada. A resultas, se simplificó sobremanera el análisis de los ritmos \parencite[46-49]{brandwood1956}. Finalmente, Brandwood decidió concentrarse en el léxico y empezó a perforar tarjetas en abril de 1960. Esto llevó a su equipo quince meses. Verificar las tarjetas y corregir los errores, preparar las cintas y ejecutar el programa, varios miles de horas \autocite[6]{burton1981}.
  
Al mismo tiempo que el equipo de Montgomery se dedicaba a Dryden, y Booth y Richens a la traducción,  Harry Josselson y Howard Hyatt estaban usando también equipos de IBM para elaborar una cuenta de palabras rusas \parencite{josselson1953} extraídas de prosa y drama de los grandes autores del siglo \textsc{xix}. Su objetivo era determinar las frecuencias de las categorías gramaticales en esa lengua y su distribución temporal y respecto a los géneros literarios. No se trataba ni de un índice ni de una compilación de concordancias, pero supuso una gran contribución para el estudio de estas últimas. Introdujeron reglas para marcar las entradas en las tarjetas y nuevas maneras de validar y ordenar los datos \autocite[4-5]{burton1981}.

Entretanto, el desarrollo y la innovación en la metodología computacional progresaba a pasos de gigante. En 1953, un equipo de investigación de la Armada de los Estados Unidos, a las órdenes de la entonces capitán de corbeta Grace Hopper, empezó a desarrollar FLOW-MATIC, un primer lenguaje de programación de alto nivel y presentó el prototipo inicial en 1955 \parencite{hopper1978}. Este avance supuso que ya no sería necesario programar los computadores en código de máquina, sino que, a partir de entonces, se podrían controlar usando instrucciones en inglés. La invención no podía ser más oportuna, pues IBM abría su negocio al público general también en 1953 con su modelo IBM 701. Aún quedaba prácticamente una década para que los lenguajes de alto nivel destronaran al código de máquina, el tiempo que se necesitó para que un público más general y sin experiencia previa en computación reemplazara a los científicos y técnicos en el perfil del usuario mayoritario.

Ajeno a lo que se estaba llevando a cabo en los laboratorios militares, el padre Busa continuaba con el método digital «tradicional», aunque los medios disponibles mejoraron sustancialmente durante la siguiente década. Esto le dio la oportunidad de expandir su empleo a otros textos y lenguas clásicas. En 1957, estableció el \textit{Centro per l'Automazione dell'Analisi Letteraria} en Gallarate, dedicado principalmente al estudio de textos en griego y latín \parencite[3]{burton1981}. La flexibilidad de la metodología del jesuita se demostró con la indexación de los \textit{manuscritos del Mar Muerto}, que habían sido descubiertos en los años en que Busa comenzaba su periplo digital. Los textos provenían de rollos en hebreo, arameo y nabateo, todos ellos idiomas semíticos con diferentes escrituras paleográficas sinistroversas \parencite{busa1958}. 

Al poco de comenzar el decenio de 1960, el Centro de Gallarate contaba con una treintena de empleados. Para 1964, este contingente ya había duplicado su número. Seis de aquellas personas se dedicaban a tareas administrativas, catorce a la preparación de textos latinos, griegos y hebreos, cuatro a programar los computadores y las casi cuarenta restantes —más de la mitad del personal— estaban encargadas de perforar las tarjetas e inspeccionarlas visualmente \parencite{busa1965}.

Precisamente, Busa expuso esto durante la Literary Data Processing Conference, que se celebró en septiembre de 1964 en las instalaciones de IBM de Yorktown Heights, en el estado de Nueva York. Esta conferencia tuvo el honor de ser el primer evento académico sobre humanidades digitales. En sus actas, ya se bosqueja una panorámica bastante fiel a los problemas que afronta la disciplina y las preocupaciones que suscita: el interés en la lexicografía, la edición textual, la enseñanza de la lengua, y la estilística, aspectos metodológicos tales como la programación y los datos de entrada y de salida, y el reto de salvaguardar los textos electrónicos \parencite[7]{hockey2004}.

A pesar del tremendo éxito de aquel jesuita del Veneto, la computación seguía restringida a unos pocos centros de estudio, aunque continuaba avanzando con paso firme. Hasta bien entrada la década de 1970, solo una minoría tenía acceso a máquinas capaces de llevar a cabo los cálculos que requerían los estudios filológicos digitales. Los investigadores podían tener la idea y la técnica podía haber desarrollado una solución, pero eso no era ni muchos menos garantía de que pudiera llevarse a cabo. El coste de los equipos continuaba siendo prohibitivo y el espacio requerido para alojarlos, desmesurado. Aquellos ingenios electrónicos requerían grandes instalaciones y personal muy especializado. Además, codificar los textos seguía demandando mucha mano de obra. De ahí que las iniciativas dependieran siempre, en gran medida, de la disponibilidad de un centro de computación y grabadores de datos. Disponiendo de todo esto, además, no debía faltar tampoco la paciencia, porque el siguiente trabajo en la cola de espera solo se empezaba a procesar una vez acabado el anterior. 

Tuvieron suerte en este sentido \citeauthor{mosteller1963}~\parencite*{mosteller1963}, pues contaron con  la ayuda técnica de un equipo de investigación de IBM dirigido por el recientemente fallecido pedagogo Albert E. Beaton. El objeto de su estudio eran unos textos de cierta importancia en la corta historia estadounidense, una colección de 77 ensayos breves que se habían publicado de manera anónima entre 1787 y 1788 en \textit{The Federalist} para persuadir a los habitantes del estado de Nueva York de ratificar la recién redactada \textit{Constitución de los Estados Unidos}. La autoría de casi todos los ensayos estaba establecida con solidez, pero doce aún permanecían disputados y, de tres escritos al alimón, se ignoraba la extensión de la contribución de cada autor.

La cuestión se venía discutiendo desde finales del siglo \textsc{xix}. A ninguno de los posibles padres de los textos en disputa le faltaron paladines dispuestos a romper cuantas lanzas fuera menester para defender a su predilecto. No hace falta decir que, en muchos casos, los argumentos se basaban en impresiones personales de los exégetas. La propuesta de \citeauthor{mosteller1963} era mucho menos susceptible de ser criticada por su subjetividad: tomaron las palabras más y menos frecuentes y compararon su uso. Esto es, verificaron si algún autor usaba léxico poco común más de lo habitual, pero también cómo se distribuían los vocablos más típicos  —tales como las preposiciones \textit{by}, \textit{from} y \textit{to}—, ya que la frecuencia de las comunes permanece más o menos invariable cualquiera que sea el tema, al contrario de lo que ocurre con las palabras de mayor carga léxica.

A la luz de las pruebas realizadas, \citeauthor{mosteller1963} encontraron una alta probabilidad de que todos los textos disputados salvo uno hubieran salido de pluma de Madison, así como las contribuciones más extensas en dos de los tres ensayos escritos en colaboración. En la conclusión de su artículo, subrayan que sus «datos complementan de manera independiente los del historiador». Observaron asimismo —y de esto se siguen aprovechando hoy en día los más modernos estudios de atribución de autoría—, que la distribución de palabras frecuentes como elemento discriminador permanece estable a lo largo de décadas y entre diferentes géneros. Habían completado el primer estudio estilométrico\index{estilometría} de atribución de autoría basado en la \textit{lectura distante}\index{lectura distante}, concepto que se presentará más adelante.

Hay que mencionar que ya Alvar Ellegård \parencite*{ellegard1962} se había adelantado al examinar las cartas que escribió Junius para criticar el gobierno del rey inglés Jorge III. No obstante, si bien Ellegård se había ayudado de la informática para llevar a cabo los cálculos estadísticos, tanto la recuperación de palabras como su cuenta y clasificación se realizó de forma manual \parencite[5]{hockey2004}.

Estos no fueron los únicos estudios estilísticos llevados a cabo durante aquellos años. No en vano, algunas propuestas de análisis estilométrico\index{estilometría} basado en la frecuencia de las palabras se habían adelantado más de un siglo a los intentos digitales. La diferencia estriba en que esos primeros análisis no tenían manera de manejar los volúmenes de texto que la introducción del computador hizo posible tratar. Por ejemplo, de acuerdo con \citeauthor{hockey2004}~\parencite*[5]{hockey2004}, ya en 1851 Augustus de Morgan había propuesto examinar la autoría paulina de las epístolas atribuidas al santo a partir del examen cuantitativo del léxico. Trae a colación \citeauthor{hockey2004} también que, más de una centuria después, el clérigo escocés Andrew Morton tomó el testigo, pero este pertrechado de los adelantos técnicos del siglo \textsc{xx}. A partir de la frecuencia las palabras más comunes y algunos cálculos estadísticos relativamente sencillos, Morton \parencite*{morton1965} concluyó que Pablo solo había escrito cuatro de las epístolas que comúnmente se le atribuyen.

También en las Islas Británicas, poco más o menos por aquellos mismos años que los otros pioneros de la filología digital, el medievalista Roy Wisbey empezó a interesarse por las posibilidades que ofrecían las concordancias para el estudio de textos en alto alemán medio. A resultas de su propia experiencia \parencite{wisbey1968}, comenzó a usar computadoras en Cambridge a comienzos del verano de 1960 para preparar índices, concordancias y material lexicográfico de textos germánicos medievales. Presentó una solicitud a su universidad para establecer un centro de computación apenas empezaba a entrar el otoño. El \textit{Literary and Linguistic Computing Centre} de la Universidad de Cambridge se estableció formalmente en 1964.

Los equipos también los proporcionaba IBM —esto no ha de extrañar, pues la competencia apenas estaba empezando a despegar—. La iniciativa británica se cuidó muy especialmente de que las máquinas fueran capaces de leer e imprimir símbolos diacríticos. A pesar de ello, el trabajo seguía siendo complicado. \citeauthor{wisbey1968} dan cuenta de que, una vez que el Centro había aceptado un proyecto, el filólogo y el programador debían de acordar cuál era el formato más adecuado para perforar las tarjetas. Esto implicaba evaluar cuidadosamente todos los signos que podían aparecer, así como potenciales fragmentos en lenguas diferentes a la principal del texto. Después de eso, había que transcribirlo usando dispositivos perforadores. Para facilitar la tarea, en lugar de hacerlo con la equipación genérica provista de todos los diacríticos, se aligeraba el mecanografiado empleando equipos ideados para diferentes idiomas modernos europeos.
 
El sistema se organizaba en torno a un programa de clasificación grabado en una cinta magnética. Mientras que la propia empresa fabricante de las máquinas proveía este programa para la mayoría de las aplicaciones comerciales, en Cambridge tuvieron que programar las máquinas ellos mismos para adaptarlo a sus necesidades particulares. A pesar de esto, descartaron la idea de un programa general de concordancias o un índice de palabras. De hecho, se rechazaba cualquier proyecto que levantara sospechas de ser ya no irrealizable, sino «muy ambicioso». Debemos pensar que, como dijimos, solo se llevaba a cabo un trabajo cada vez y este podía tomar una gran cantidad de tiempo en terminarse, durante el cual, cualquier otro proyecto debía permanecer a la espera.

Los recursos se empleaban eminentemente para trabajos de carácter lexicográfico, centrados casi exclusivamente en textos europeos medievales —alemán, español, francés, inglés y neerlandés—. Contando únicamente los textos en alto alemán medio, la suma ascendía a tres cuartos de millón de palabras. Entre los proyectos que se llevaban a cabo en el Centro, estaban índices de concordancias de dos ediciones diferentes de un mismo texto, que proporcionaban ediciones duales; un diccionario de rimas del alemán, junto a un programa capaz de identificar pares de rimas en un texto; un generador de frases a partir de una gramática; un comparador de textos que encuentra pasajes divergentes. El fruto más jugoso de aquel árbol fue quizás el \textit{Vollständige Verskonkordanz zur \emph{Wiener Genesis}} \parencite{wisbey1967}.

En el continente, destacan las digitalizaciones que se llevaron a cabo en el \textit{Centre de recherche pour un Trésor de la langue française}. El centro se estableció en la Universidad de Nancy en 1960 bajo el auspicio de su entonces rector Paul Imbs para crear un diccionario de la lengua francesa a la imagen del \textit{Oxford University Dictionary}. La digitalización comenzó en 1963 con equipos Gamma 60 de la firma francesa Bull, pero, si bien la digitalización fue crucial en la composición de la obra, no se puso todo ese potencial a disposición del público hasta 1992, año en que se publicó la primera edición en CD-ROM\footnote{No podemos dejar de mencionar que aquellas computadoras Gamma 60 entraron en el imaginario de la cultura popular por el papel que una de ellas representó en la película del recientemente fallecido Jean-Luc Godard \textit{Alphaville}. Una máquina idéntica a las que, en aquel tiempo, ya estaban recopilando datos lexicográficos en Nancy, convenientemente rebautizada como Alpha 60 para la ocasión, encarnaba —en la medida en que una máquina es capaz de encarnar— al antagonista cibernético del hierático detective Lemmy Caution.} \parencite{papelier2005}.

La década de 1960 no solo fue productiva en el ámbito lexicográfico, sino también tremendamente prolífica en otros muchos aspectos de la computación aplicada a las humanidades. En 1963 se presentó TUSTEP, la primera herramienta para la edición crítica filológica de textos. Este programa, abreviatura de \textit{TUebinger System von TExtverarbeitungs-Programmen}, fue ideado por un equipo dirigido por el teólogo y filólogo Wilhelm Ott en la Universidad de Tubinga \parencite[7]{hockey2004}. Hoy sigue en desarrollo activo, con su última versión \parencite{tustep2023} publicada en el año que se redactaron estas líneas y cuenta con una fiel comunidad de usuarios\footnote{\citeauthor{valdes2014b}~\parencite*[p.~39, n.~43]{valdes2014b} apunta a la causa que ha podido relegar este sistema a una posición secundaria en el mercado internacional: los manuales y documentación se encuentran exclusivamente en alemán.}.

La informática aplicada a lo humanístico, si bien alejada aún décadas de la madurez de las humanidades digitales, había alcanzado ya un punto de desarrollo notable, lo que propició también la reflexión sobre la propia disciplina y en el marco de esta. Esto fructificó en el nacimiento de la primera publicación científica dedicada específicamente a este ámbito. Se tituló \textit{Computers and the Humanities} y su primer número vio la luz en septiembre de 1966. En su presentación leemos el siguiente desiderátum:

\blockquote{\begin{english}We recognize the strong fear often expressed that machinery will destroy the intangibles of humane scholarship—the intuitive, subtle responses of the trained mind. Distinguished voices have already warned that a fascination with method may seduce us away from meaningful goals. Such warnings are well taken, and our editorial limits will constantly exclude technique for its own sake. We need never be hypnotized by the computer's capacity to count into thinking that once we have counted things we understand them.\end{english} \parencite{prospect1966}}

Aquella declaración de intenciones sigue sonando actual más de medio siglo después. A nadie familiarizado con el tema le llamaría la atención encontrar tal descargo en las más recientes publicaciones, ya que en ellas no suelen faltar otros en idénticos o muy semejantes términos. El tiempo no parece haber hecho mella en la necesidad de los detractores más viscerales de las humanidades digitales de hacer críticas, fundadas o no, ni moderado la querencia de sus practicantes por abrir el discurso con prevenciones de tal índole —más por el deseo de apaciguar a aquellos que por llamar la atención sobre algo de tal obviedad que a pocos se les escapa—. Para bien o para mal, estos fenómenos siguen jalonando ambos lados del sendero por el que discurren disciplinas que, para buena parte de sus practicantes, se definen

\blockquote{\begin{english}not just by their subject domains, but by a specific set of methods, emphasizing qualitative description of selected case studies. If we start from this assumption, it will look like a definitional error to describe new quantitative projects as an expansion of the humanities. At best, we will be able to say that research using numbers dilutes the distinctive character of the humanities; at worst, we will fear it as a Trojan horse subverting the humanities' true purpose, which is presumably to serve as a counterweight to science.\end{english} \parencite{underwood2019}} 

		Volviendo a la relación de hitos, en la década de 1970, surgieron centros de computación asociados a academias europeas de diversas lenguas con el fin de facilitar el trabajo lexicográfico en la composición de diccionarios, como el \textit{Instituut voor Nederlandse Lexicologie} en Leiden, dedicado al neerlandés \parencite[4]{hockey2004}. Los vagos nexos que relacionaban los dispersos esfuerzos investigadores tomaron cuerpo en forma de asociaciones científicas, creadas con el fin de promover el desarrollo de la \textit{humanities computing}\index{humanities computing@{\textit{humanities computing}}}. Así, la \textit{Association for Literary and Linguistic Computing} se conformó en 1977 y, un año más tarde, la \textit{Association for Computers and the Humanities} \parencite[8]{hockey2004}.

Tal vez, el desarrollo más notable de la década no fue en lo que se podía hacer con las máquinas, sino en cómo llevarlo a cabo. Los investigadores habían tomado conciencia sobre la existencia de otros proyectos similares a los suyos y que esto llevaba no pocas ocasiones a la duplicidad de esfuerzos. Con esta premisa en mente, se desarrolló en Gran Bretaña la segunda revisión \parencite{berry1973} de COCOA \parencite{russell1967}, un programa cuya primera versión había presentado la Universidad de Cardiff a finales de la década anterior. La novedad conceptual de la nueva reinterpretación era que no estaba diseñada para cumplir un único propósito en un equipo determinado, sino que podía aceptar diferentes corpus y funcionar en equipos distintos. Esta necesidad de aprovechar el trabajo llevó a crear el Oxford Text Archive en 1976, una primera biblioteca digital dedicada a recopilar y mantener los textos digitales empleados en las investigaciones y ponerlos a disposición de otros investigadores. A la iniciativa de Oxford, hay que sumar otras colectivas, como el \textit{Thesaurus Linguae Graecae} (TLG), a expensas de la Universidad de California Irvine, que más tarde se amplió con una base de datos de textos latinos \parencite[8-9]{hockey2004}.

De la década de 1970, resultan reseñables asimismo dos aspectos relacionados indirectamente con \textit{humanities computing}\index{humanities computing@{\textit{humanities computing}}}. Por una parte, la entrada en escena de los discos magnéticos como sustitutos de las cintas —que habían reemplazado a su vez a las tarjetas perforadas—  para el almacenamiento de datos. Esto supuso que ya no fuera necesario procesar los datos de manera secuencial, lo que agilizó el empleo de los computadores. Piénsese que, hasta entonces, la información se almacenaba en cintas, cuyos principios y mecanismo no diferían mucho de las cintas de audio o de vídeo domésticas que todos conocemos. Por lo tanto, del mismo modo que no se requería pasar toda la cinta de música sin perder la vista el contador de vueltas para llegar a la canción deseada y rebobinarla por completo para escucharla desde el principio, también era menester pasar de manera mecánica por toda la cinta de datos hasta llegar a los buscados. La irrupción del disco magnético, al igual que sucedió después con el disco compacto en la industria musical, eliminó de un plumazo esta limitación. Por otra parte, empezaron a impartirse cursos sobre computación aplicada a las humanidades e incluso se consideraron los lenguajes de programación como ejercicios de disciplina mental al modo que se había usado tradicionalmente el latín \parencite[9]{hockey2004}.

La siguiente década supuso una revolución en la difusión y alcance de la informática. Esta se popularizó gracias a que las máquinas habían reducido su precio y tamaño y, a la vez, aumentado su capacidad. Microcomputadores, como el Sinclair ZX Spectrum o el Commodore 64, entraron en muchos hogares y el IBM PC vio una implantación en el ámbito profesional como ningún otro computador lo había hecho antes \parencite{fradejas2022a}. Aquellos trabajos que, hasta hacía nada, eran prerrogativa de los pocos afortunados que contaban con el respaldo de un centro de computación, se hallaban ahora accesibles a cualquiera. Al mismo tiempo, los conocimientos técnicos necesarios para operar un computador y los recursos humanos para hacerlo se habían reducido a velocidad pareja a la de los costes. Ya no era necesario contar con un equipo de trabajadores para perforar tarjetas durante meses porque una sola persona podía introducir directamente los datos valiéndose del teclado; tampoco se requerían programadores especializados capaces de interactuar con la máquina en su propio código digital, sino que los lenguajes de alto nivel con los que venían equipados los computadores\footnote{Nótese que el usuario de los tres populares microcomputadores mencionados interactuaba por efecto con la máquina mediante comandos del lenguaje BASIC.} franqueaban el paso a los legos.

Hay que señalar un hito que aconteció en 1987. Ese año se organizó una reunión en el Vassar College de Poughkeepsie para evaluar la posibilidad de definir un estándar de codificación humanística de textos. Hasta entonces, los intentos de unificar formatos habían sido infructuosos. Un año antes se había presentado un nuevo método de codificación a instancias de la \ac{iso} denominado \ac{sgml}\index{{Standard Generalized Markup Language}!see SGML}, siglas de \textit{Standard Generalized Markup Language}. Este formato era ideal para codificar una gran variedad de textos, incluyendo metadatos, lo que abría la puerta no solo a la representación del texto, sino a la inclusión de aquella información adicional que requerían los investigadores \parencite[12-13]{hockey2004}. Aquello fue el germen de la Text Encoding Initiative, más conocida por sus siglas \ac{tei}. La primera edición de sus directrices se publicó en mayo de 1994 y varios proyectos las adoptaron poco después. Apenas había comenzado 1999, cuando la Universidad de Virginia y la Universidad de Bergen presentaron una propuesta conjunta al comité ejecutivo de \ac{tei} para la creación de la organización internacional \ac{tei} Consortium para mantener y desarrollar el estándar \ac{tei}. La iniciativa noruego-estadounidense fue aceptada y pronto se unieron al consorcio la Universidad de Brown y la Universidad de Oxford. La prioridad de la recién creada organización era presentar una versión \ac{xml} de las directrices, lo que cuajó a mediados de 2002. Estas eran una mejora de las primigenias, sin cambios sustanciales salvo la propia codificación y la resolución de algunos errores que se habían detectado. En 2007 se revisaron a fondo las directrices y se presentó la versión actualizada que hoy sigue en uso \parencite{tei2005}.

De acuerdo con \citeauthor{rio2015}~\parencite*[3]{rio2015}, fue la década de 1990 la que vio las primeras aproximaciones a la computación aplicada a las humanidades en el mundo hispanohablante. Da fe de ello la obra de orientación eminentemente filológica de \citeauthor{marcos1994}~\parencite*{marcos1994}. Se lamenta  (pp. 55-70) de la escasa atención que se le ha prestado a la innovación lingüístico-tecnológica desde las instancias oficiales, aunque reconoce varios proyectos surgidos al amparo de la largueza financiadora que propiciaron los fastos del quinto centenario del descubrimiento de América. Llama la atención sobre la necesidad de compilar un corpus del español estandarizado conforme a \ac{tei} \autocite[79-90]{marcos1994}. No fue a caer en saco roto, pues, en 1995 \parencite[197]{rojo2016}, se aprobaba la creación del Corpus de Referencia del Español Actual de la \acl{rae} (RAE), que se desarrolló a una velocidad inusitada \parencites{pino1995}{pino1996}. Desde entonces, la disciplina no ha dejado de avanzar. 

Alrededor de 2004, aconteció un cambio terminológico con implicaciones conceptuales profundas, para visibilizar un nuevo paradigma. Lo que se había dado en llamar \textit{humanities computing} pasó a ser uno de los muchos aspectos de un concepto mucho más amplio que se bautizó como \textit{digital humanities} o, como las conocemos en español, humanidades digitales. A la popularización del término contribuyó la influyente antología monográfica editada por \citeauthor{schreibman2004}~\parencite*{schreibman2004}. El motivo de  esta transformación era prevenir la concepción de la disciplina como el mero empleo de herramientas digitales para acelerar el estudio tradicional. Se trataba ahora de expandirla para tratar con entidades digitales desde la perspectiva humanística y no con entidades humanísticas con herramientas digitales. Esto es, «\begin{english}they point toward a new way of working with representation and mediation, what might be called the digital ‘folding’ of reality, whereby one is able to approach culture in a radically new way\end{english}» \parencite[1]{berry2011}. Ironías del destino, ese mismo año también dejaba de publicarse la histórica \textit{Computers and the Humanities} \parencite[6]{crymble2021}, como queriendo ratificar así la nueva tendencia. Dicho de otra manera, con este término, la disciplina pretendía trascender su estatus de servicio complementario de apoyo y erigirse como una actividad intelectual por derecho propio, rigurosa y con un universo teórico por explorar \parencite[43]{hayles2011}.

\section{Datos}\index{datos}
En el curso de estas contextualizaciones metodológica e histórica, hemos nombrado en varias ocasiones el término \textit{datos}, pero hemos eludido ofrecer una definición precisa, confiando en el buen entendimiento del lector, pues es una noción de corriente en un mundo cada vez más digitalizado. Llegados aquí, la idea intuitiva se antoja insuficiente y se hace necesario precisar a qué nos referimos cuando hablamos de datos porque el concepto se materializa de muy diversas maneras en función de su propósito.

Mientras que los datos son un concepto ubicuo en las ciencias sociales —las encuestas con las que se elaboran las estadísticas con las que nos desayunamos a diario, por ejemplo—, siguen resultando \textit{rara avis} en las humanidades: su empleo dista de poder darse por supuesto. Los métodos que usaba la disciplina hasta hace poco casi en exclusiva no se prestaban a clasificaciones formalmente estables, por lo que las nuevas aproximaciones han traído consigo requerimientos metodológicos y estos, a su vez, interpretativos. Quizá la recolección de datos sea uno de los puntos que más han influido en esta tendencia, ya que requieren gran cantidad de tiempo y recursos \parencite[41]{ehrlicher2019}.

La extracción de datos es precisamente el pilar donde se sustenta el proceso, pues la fiabilidad del análisis depende de la calidad de los datos, como bien sugiere la máxima informática \textit{garbage in, garbage out}. Inversamente, la calidad de los datos producidos condiciona su interpretación. Como extractores y organizadores de esos datos, no podemos ignorarlo. De nada sirven unos algoritmos impecables si producen datos en un formato ilegible y, al contrario, esos mismos algoritmos poco tienen que ofrecer si los datos de entrada son irrelevantes, erróneos o intratables. Veamos entonces cómo abordaremos la cuestión.

Una primera aproximación al concepto \textit{datos} la hacemos definiéndolos como la «información, en cualquier forma, con la que opera un computador» \parencite[\textit{s.v.} \textit{data}]{butterfield2016}, que no se aleja mucho de nuestra noción intuitiva del significado del término. Desde una perspectiva etimológica, \textit{data} es el plural de \textit{datum}, que es el género neutro del participio pasado del verbo \textit{dare}.  O sea, `dado' o, nominalizado, `[algo] dado'. En el siglo \textsc{xvii} se empezó a emplear en cálculos matemáticos para indicar presupuestos \textit{dados} \parencite[\textit{s.v.} \textit{data}]{onlineetymology}. Sin embargo, este uso se extendió  en la segunda mitad del siglo \textsc{xx} para adquirir nuevas connotaciones acordes a los nuevos tiempos. De esta manera, en la década de 1960 algunos sectores concretos de la administración estadounidense ya definían datos como

\blockquote{un término general empleado para denotar alguno o todos los números, letras y símbolos, o hechos que se refieren a o describen un objeto, idea, condición, situación u otros factores. Connota elementos básicos de información que pueden ser procesados o producidos por un computador. En algunos casos, se considera que los datos solo pueden expresarse de numéricamente, pero la información no está así limitada. \parencite[\textit{s.v.} \textit{data}; traducción propia]{bureau1962}}

Esto conduce al término \textit{información}, que no es otra cosa que una «colección de hechos u otros datos, especialmente los derivados del proceso de datos» \parencite[\textit{s.v.} \textit{information}]{bureau1962}. Obviando la circularidad de la definición, parece que estamos ante dos conceptos relacionados, ¿pero cuál es esta relación? Una manera simple de entenderlo es hablar del «sentido que un ser humano concede a los datos»  \parencite[\textit{s.v.} \textit{information}; traducción propia]{government1997}. Tal vez una descripción más conveniente para nuestro propósito sea la que da la siguiente fórmula: $informaci\acute{o}n = datos + estructura$ \parencite[\textit{s.v.} \textit{data}]{bradley2004}, por lo que, despejando la ecuación, datos serían información sin estructura.

Los datos pueden ser \textit{cuantitativos}\index{datos!cuantitativos} o cualitativos. Cuantitativos, como su nombre indica, son los susceptibles de contarse que proporcionan información a través de su tratamiento matemático: el número de palabras de un texto; de estas, cuántas son léxicas y, de estas  últimas, cuántas son adjetivos\index{adjetivo} femeninos singulares, por ejemplo. Los datos \textit{cualitativos}\index{datos!cualitativos}, por el contrario, son aquellos otros que, no siendo susceptibles de cuantificación, permiten hacer valoraciones interpretativas descriptivas: la calidad del texto, su ambientación, la profundidad psicológica de sus personajes... En el punto medio, tenemos datos categóricos si, por ejemplo, una palabra es léxica o un adjetivo femenino singular. Este tipo de clasificación depende en gran medida de la persona que juzga los datos. En cualquiera de los dos casos, hemos resuelto la ecuación anterior, si bien el primero y el último se estructuran mediante algoritmos formales y el segundo solo admite una interpretación holística.

Así pues, atendiendo a la posibilidad de definiciones categóricas, no debemos confundir lo digital con lo cuantitativo. Es cierto que el poder de cálculo del computador lo hace singularmente apto para contar y llevar a cabo operaciones aritméticas, pero recordemos por un momento su denominación \textit{ordenador}: el computador ordena datos y los presenta de manera propicia para su interpretación cualitativa. Y, al contrario, la metodología cuantitativa no requiere necesariamente de asistencia digital\footnote{Hoy en día, la frontera se difumina por la omnipresencia del computador en todos los ámbitos. Como muestra de la dificultad para trazar una línea, considérese, por ejemplo, el reciente estudio de \citeauthor{aichinger2023}~\parencite*{aichinger2023}. En este, los datos se extrajeron manualmente, aunque el computador sirvió de herramienta para presentarlos mediante una hoja de cálculo y llevar a cabo en ella sencillas maguer repetitivas operaciones aritméticas. Cuando la diferencia, como en este caso, no es metodológica, sino instrumental, ¿cabría hablar de \textit{análisis digital}? ¿Dónde ponemos la frontera?}, como ya habíamos visto antes cuando hablábamos de los precedentes históricos de la estilometría\index{estilometría} de nuestros días\footnote{Tenga el lector esta prevención en cuenta  y sepa disculpar el pequeño sacrificio de exactitud que haremos en este trabajo en aras de la conveniencia al emplear en ocasiones el término \textit{cuantitativo} para referirnos específicamente a aquellos métodos asistidos por computador.}.

\input{tables/schubart} 

La cuantificación existe desde mucho antes de la existencia del computador digital. Si bien es innegable que se ha usado de manera cuando menos discutible, valga como ejemplo la tabla del médico y poeta Mark Akenside bajo seudónimo Musiphron \parencite*{musiphron1746}, en la que presenta una escala de poetas europeos de todas las épocas siguiendo el ejemplo de la que compuso Roger de Piles \parencite*[489-498]{piles1708} para pintores (franceses en su mayoría). Algunas de las magnitudes, como las que se refieren al colorido o el dramatismo suscitan la cuestión de cómo se cuantificaron (\Cref{tab:schubart}). A pesar de todo, hay que reconocer que la subjetividad de estas valoraciones palidece ante la escala que propuso el poeta y músico Christian Friedrich Daniel Schubart para los \textit{jóvenes} poetas alemanes, en la que consideraba aspectos como el \textit{genio} o la \textit{gracia} \parencite{schubart1792}\footnote{Ver \citeauthor{spoerhase2018}~\parencite*{spoerhase2018} para profundizar en estos y otros ejemplos de escalas de este tipo en el siglo \textsc{xviii}.}.

No todos los datos cuantitativos se organizan de igual manera. Se distinguen según su grado de estructuración, que, además, resulta ser un factor determinante del modo en que habremos de tratarlos. De esta forma, se habla de datos \textit{estructurados} y \textit{no estructurados}. La denominación de estos últimos\index{datos!no estructurados} surge como parte de la jerga especializada de las bases de datos para referirse a aquello que no podía guardarse en formato tabular, o sea, \textit{estructurado}. Se denominan, pues, datos no estructurados a los que presentan ambigüedades clasificatorias que hacen inviable su procesamiento digital, aquellos que carecen de una clasificación lógica mediante compartimentaciones explícitas \parencite[15]{brackett2014}. Esto es, por ejemplo, una imagen o un texto digital. Pensemos en una obra de teatro del Siglo de Oro como la que corresponde al fragmento de la derecha de \Cref{fig:test} en un procesador de texto. El lector discierne una estructura clara en la disposición espacial de los elementos de la izquierda si conoce ciertas convenciones. Sabrá que las letras en versalita sin sangrar corresponden a una didascalia de personaje, el texto sangrado  corresponde a parlamentos y cada línea se relaciona con un verso, excepto si están en cursiva, en cuyo caso se trata de una acotación. El cerebro ha procesado las marcas visuales y ha estructurado la información.

\begin{figure}[!ht]
	\centering
	\footnotesize
	\begin{subfigure}{.45\textwidth}
		%\LMR
		\textsc{INOCIENCIA}\tabto{7.5em}Que el Hombre rompido ha
		
		\tabto{7.5em}la cadena, y que se va.
		
		\textsc{TEOS}\tabto{7.5em}La Verdad enternecer
		
		\tabto{7.5em}podría el yerro, mas romper
		
		\tabto{7.5em}la prisión no.
		
		\textsc{INOCIENCIA}\tabto{9em}Claro está
	\end{subfigure}%
	\begin{subfigure}{.40\textwidth}
		\texttt{INOCIENCIA{\DVS↹}Que␣el␣Hombre␣rompido␣ha {\DVS⏎↹}la␣cadena,␣y␣que␣se␣va.{\DVS⏎}TEOS{\DVS↹}La␣ Verdad␣enternecer{\DVS⏎↹}podría␣el␣yerro, ␣mas␣romper{\DVS⏎↹}la␣prisión␣no.{\DVS⏎}INOCIEN CIA{\DVS↹↹}Claro␣está}
	\end{subfigure}
	\caption{Representación visual y cadena de caracteres.}
	\label{fig:test}
	\normalsize
\end{figure}

Desde el punto de vista del computador, sin embargo, el texto es algo como lo que aparece a la derecha de \Cref{fig:test}. Ni siquiera eso, porque lo que vemos en pantalla no es sino una forma de representar una serie de grupos de bits, cada uno con la codificación adecuada para cada carácter, desde $01100001$ para la letra \textit{a} hasta $00001010$ para el salto de línea. Incluso esta notación no deja de ser una abstracción porque, en realidad, tampoco son ceros y unos, sino estados de carga eléctrica o magnética. Olvidémonos de estas concreciones y volvamos a las representaciones abstractas. Letras, números, símbolos o espacios en blanco son todos un elemento de la misma clase. Si queremos dividir por palabras, por ejemplo, la máquina necesita ser instruida para reconocer los caracteres alfabéticos, encontrar los límites de cada agrupación (espacios, signos de puntuación, etc.) y ordenar las divisiones resultantes en una lista para construir así una estructura de orden superior.

Los datos estructurados son aquellos que están ordenados de acuerdo un patrón específico y sus elementos tienen relaciones susceptibles de ser representadas en formato tabular. En este caso, podría tomarse cada línea de texto, asignarle una fila, e ir añadiendo la información adicional conocida en nuevas columnas\footnote{En el ejemplo, modernizamos la didascalia de personaje para tener un locutor normalizado si deseamos comparar con otras obras.}.

\input{tables/datosestructurados}

Aun conteniendo exactamente los mismos grafemas\index{grafema}, estos datos estructurados ya no son la fuente de \Cref{fig:test} porque, durante el proceso de categorización, hemos hecho juicios al considerar unos criterios y descartar otros \parencite[21]{drucker2021}. Como consecuencia, se ha perdido por el camino información: en el ejemplo, incluso con todos los grafemas intactos, ha desaparecido toda la composición tipográfica, desde el tipo de letra al espaciado. Por una parte, esto refleja lo incompleto del modelo y, por otra, pone de manifiesto que los datos son tan objetivos como lo haya sido su selección. En otras palabras, el límite de la objetividad de una investigación digital está limitado por la que sea capaz de alcanzar el propio investigador.

A las dos categorías de datos vistas, habría que añadir una tercera intermedia, a la que se denomina datos \textit{semiestructurados}\index{datos!semiestructurados}. En realidad, se considerarían como una modalidad diferente de datos estructurados, pues se definen como aquellos en los que el esquema estructural es intrínseca; son, por así decirlo, autodescriptivos. Por el contrario, en los datos estructurados en el sentido clásico, la estructura y los datos son entidades diferentes. Para  ilustrarlo, recurriremos al formato semiestructurado por excelencia, \ac{xml}, en conformidad con las directrices de \ac{tei}. El \Cref{list:xml}, como vemos, presenta una disposición jerárquica en la que las etiquetas delimitan los niveles, los metadatos como el número de verso u otras de sus propiedades, como ser una parte de un verso compartido, se marcan como atributos de la etiqueta del texto al que conciernen\footnote{En este caso, la didascalia de personaje no se normaliza, pero sí el identificador único. Esto lo veremos en más detalle en próximos capítulos.}. 

\begin{lstlisting}[numbers=none, frame=none, keywordstyle=\ttfamily,  language=xml, upquote=true, label={list:xml}, caption={Datos semiestructurados.}]
<sp who="#inocencia">
    <speaker>INOCIENCIA</speaker>
    <l n="1030">Que el Hombre rompido ha</l>
    <l>la cadena, y que se va.</l>
</sp>
<sp who="#teos">
    <speaker>TEOS</speaker>
    <l>La Verdad enternecer</l>
    <l>podría el yerro, mas romper</l>
    <l part="I">a prisión no.</l>
</sp>
<sp who="#inocencia">
    <speaker>INOCIENCIA</speaker>
    <l part="F">Claro está</l>
</sp>
\end{lstlisting}
En lo tocante a este trabajo, damos por sentado que el grueso de las obras que se hallan disponibles se encuentra como datos no estructurados, por lo que se hace necesario prepararlos previamente. En el futuro, tal vez deje de ser este necesariamente el caso, dado el creciente número de ediciones en \ac{xmltei}, como las del proyecto DraCor \parencite{fischer2019}.

\section{Metodología \textit{dirigida} y \textit{asistida} por datos}
Ya con una idea más precisa de a qué nos referimos cuando hablamos de datos, intentaremos una clasificación metodológica más fina según la forma de emplearlos. La parte práctica de este trabajo no basa sus resultados en el procesamiento o el análisis de los datos, sino que pretende producirlos como resultado de aplicar a los textos los conceptos teóricos vistos en estos primeros capítulos. No se trata tanto de definir en esta sección cómo nos conduciremos en la segunda parte del trabajo, sino de esbozar sus fines últimos, los posibles usos potenciales de los resultados. Solo así conoceremos qué es lo que debemos producir. No perdamos de vista que nos proponemos extraer y ordenar datos de textos teatrales y el interés teórico del proceso sirve a un fin ulterior: pretendemos que esa labor sea un medio que conduzca a resultados aplicables a la investigación. Siendo así, no podemos obviar este aspecto, ya que sería tanto como ignorar las necesidades de aquello a lo que, en último término, está orientado todo el esfuerzo. 

¿Cómo pueden valerse las investigaciones humanísticas de este trabajo? La respuesta determinará los datos que debemos extraer y cómo los organizamos una vez extraídos. Contestarla pasa por especificar las metodologías predominantes para abordar la evaluación de los datos. \citeauthor{escobar2021}~\parencite*[7-8]{escobar2021} hace una primera distinción entre dos maneras de utilizarlos. Por un lado, define la metodología dirigida por datos\index{data-driven@\textit{data-driven}}\footnote{\textit{Data-driven methodology} en inglés.} como aquella en que el computador procesa los datos de entrada y produce otros de salida, fruto de la lógica y representaciones formales. Esta metodología apuesta por producir resultados objetivos y reproducibles bajo las mismas condiciones, al modo de las ciencias naturales.

Según \citeauthor{escobar2021}, en las metodologías dirigidas por datos\index{data-driven@\textit{data-driven}}, estos se usan para responder preguntas mediante la creación de representaciones formales de una cuestión y la automatización del procedimiento para llegar a responderla. Además, los criterios con los que se evalúa el resultado se definen antes de comenzar el experimento. Esta aproximación presenta el inconveniente de requerir una simplificación que, necesariamente, ha de dejar de lado muchos aspectos del referente real. Volvemos al problema de la incompletitud del modelo. Sin embargo, concentrarse en puntos concretos permite evaluarlos a una escala imposible desde una aproximación holística. Podemos comparar el ritmo de millones de octosílabos en lugar de dos romances de forma fragmentaria. Esto, además, abre la puerta a descubrir aspectos contraintuitivos, presenta la ventaja epistemológica de dar pie a experimentos reproducibles y producir resultados falsables. Planteamos la pregunta de cuál es la probabilidad de que una hipótesis dada sea cierta; otros experimentos han de poder corroborar el resultado de forma independiente. 

Las metodologías \textit{asistidas por datos}\footnote{\textit{Data-assisted methodology} en inglés.} ofrecen al investigador información para especular e interpretar desde la subjetividad. Se presentan nuevas perspectivas para el problema, abren otras vías de pensamiento o refuerzan o debilitan las ya existentes, pero no dan respuestas. Dicho de otra manera, esta aproximación aporta un punto de vista sobre la cuestión y contribuye así a formar la opinión de quien ha de encontrar los resultados. Permite incluso evaluar el problema en otro marco de referencia. Estas metodologías se prestan especialmente para problemas que son incuantificables por su propia naturaleza. Aquí no se plantea una cuestión a resolver con los datos, sino que estos se usan para matizar la respuesta o plantear nuevas cuestiones \parencite[7-8]{escobar2021}. En este tipo, se encuentran, por ejemplo, aquellos estudios que recurren a bases de datos para encontrar ocurrencias de un caso particular y las analizan sin que la electrónica intervenga en la interpretación. Tales investigaciones tienen una influencia decisiva en las humanidades digitales, especialmente en cómo se definen los datos para ser interpretados.

Las investigaciones dirigidas por datos tienen la facultad de influir en su contrapartida asistida, en tanto que condicionan la preparación de los datos para un determinado uso, lo que orientará las posibilidades de consulta, pero también tiene la capacidad de producir resultados capaces de modelar qué nuevas preguntas han de plantearse. El caso contrario es igualmente cierto porque la selección de aspectos textuales de interés, la demanda de poner la atención en uno u otro particular, suele preceder al estudio digital. Esto fuerza elecciones ontológicas cuyas consecuencias trascienden el efecto inmediato, lo que se observa muy claramente, por ejemplo, con la ayuda de visualizaciones, a las que Moretti~\parencite*[86]{moretti2019} concede la categoría de definitorias de la disciplina
\blockquote{\begin{english}because visualization is never just
	visualization: it involves the formation of corpora, the definition of data,
their elaboration, and often some sort of preliminary interpretation as
	well [...] What interests us is visualization as a practice, in the conviction that practices—what we learn to do by doing, by professional habit,
	without being fully aware of what we are doing—often have larger theoretical implications than theoretical statements themselves. Whether this has indeed been the case for digital humanities, is for readers to decide.\end{english}}

Dependiendo del proyecto, una de estas dos metodologías ofrece más posibilidades que la otra, pero también tienen demandas diferentes. Así, según \citeauthor{escobar2021}~\parencite*[12]{escobar2021}, la metodología dirigida por datos requerirá una definición precisa de los conceptos a observar que, en el caso de los estudios literarios, se traduce generalmente en elementos representados mediante valores discretos; por otra parte, estos valores deben aceptar una aproximación aritmética. Asimismo, los datos deberían poder acumularse para posibilitar la obtención de un corpus más grande a partir de la combinación de otros más pequeños. Naturalmente, esto último solo atañe a aquellos casos en los que tiene sentido, por ejemplo, para examinar la obra completa de un autor como suma de sus trabajos individuales.

De esta manera, tenemos dos formas entreveradas de abordar el objeto de estudio, ambas se influyen mutuamente y su síntesis traza las líneas directrices que señalan el devenir de la disciplina. ¿Pero cómo se aplica esto a la crítica literaria?

\section{Filología digital}
Como dijimos, cada rama de las humanidades aplica los métodos digitales en respuesta a sus necesidades particulares. Para usar una terminología adecuada, procesa un tipo determinado de datos. Para el filólogo, claro está, esos datos son los textos y, en nuestro caso particular, los textos literarios de un determinado género y una época concreta. Por otra parte, podríamos enfocarnos en otros aspectos, por ejemplo, en una caracterización puramente lingüística \parencite{sanz2021c}.

 En este sentido, los filólogos debemos considerarnos privilegiados porque, dentro de las humanidades digitales, nuestra disciplina es posiblemente la más antigua y la que más ha avanzado. La lingüística computacional y de corpus, el \acl{pln} (PLN), las ediciones digitales, los programas de reconocimiento óptico de caracteres... La suma de todo ello conforma una sólida base donde se sostienen nuevos estudios de la rama. Para diferenciarlo de las humanidades digitales en su acepción más amplia, nos referiremos a estos aspectos concretos que nos conciernen con un término más específico: \textit{filología digital} \parencite{andrews2013}.

Debemos tener en cuenta también que la filología digital no es un bloque monolítico, pues se ramifica en proyectos dedicados a la clasificación de textos,  a su recopilación, al análisis y a la difusión \parencite{baycheng2017}. En muchos casos, se trata de la extensión natural de los procedimientos tradicionales, al modo de las ediciones digitalizadas o, en los análisis, la aplicación de la tecnología a ideas que se venían gestando desde mediados del siglo \textsc{xix}, pero cuyo empleo continuó siendo muy trabajoso hasta el advenimiento y popularización de la informática \parencite[517]{hoover2013}.

Difícilmente le sorprenderá a alguien la afirmación de que la crítica literaria se remonta a los inicios de la producción textual. Las obras clásicas, sin ir más lejos, se compusieron en el marco de un campo literario dominado por una estética no solo descriptiva sino también prescriptiva. Las bondades de un texto se medían en función de su observancia de las normas tenidas por canónicas en cada momento histórico. Esto se tradujo asimismo en la aparición de métodos para evaluar el grado de fidelidad a la norma que, a su vez, servían también para comparar obras entre sí. Piénsese, por ejemplo, en la métrica en el griego clásico y la distribución de los pies no ya como criterio clasificatorio, sino como \textit{norma} determinada por el género o determinante de este. \citeauthor{rommel2004}~\parencite*[88-89]{rommel2004} señala que precisamente ese propósito comparatístico era el que tenían los estudios literarios asistidos por computador primigenios en las décadas de 1960 y 1970: buscar e identificar patrones y cadenas en los textos electrónicos. Y no olvidemos que una década antes ya habían empezado a florecer las listas de concordancias, que supusieron sin duda una gran ayuda para la lectura crítica de los textos que cubrían.

Continúa \citeauthor{rommel2004} llamando la atención sobre la enorme ventaja que supone el empleo de la electrónica, incluso tomándola como una mera herramienta carente de capacidad analítica intrínseca. Esto es, como instrumento auxiliar en estudios asistidos por datos\index{data-assisted@\textit{data-assisted}}, como hemos descrito arriba. La máquina complementa la memoria del humano, pues constituye un archivo ordenado de ocurrencias según los criterios que se le indiquen. Al contrario que los archivos físicos, el medio digital es capaz de almacenar una cantidad prácticamente ilimitada de información sin ocupar espacio. Además, puede recuperar un volumen descomunal de esta antes de que un humano tenga tiempo siquiera de abrir las tapas del ejemplar impreso y buscar el índice \footnote{Estrictamente hablando, sí ocupa espacio, aunque sea apenas los escasos 1,65 cm${}^{2}$ de una tarjeta de memoria microSD. La capacidad tampoco es ilimitada, pero, a efectos prácticos, los dispositivos de almacenamiento modernos satisfacen con creces la demandas del corpus literario más exigente.}. Por otra parte, si la máquina sirve como archivero virtual para sacar registros, no es menos útil como copista. Recorre el corpus incansable y archiva ocurrencias a una velocidad vertiginosa sin importarle el tamaño del texto o el número de ejemplares a escrutar. Estos dejan así de ser un factor limitador del trabajo humano. 

La capacidad extendida de análisis ha trascendido su esencia original a nuevas aproximaciones comparatísticas gracias a la disponibilidad masiva de fuentes primarias procesables. Esta nueva suerte de entender las humanidades digitales ya no supone solo un cambio cuantitativo respecto al volumen de texto y el tiempo empleado, sino que la aplicación de la tecnología a la crítica puede adquirir una naturaleza cualitativa. Trasladamos aquí la apreciación de Moretti \parencite*[117]{moretti2008} sobre la diferencia entre la novela china y europea, «a story with a thousand characters is not like a story with fifty characters, only twenty times bigger: it's a different story». No se trata de aplicar sin más nuevas aproximaciones que, incluso estando imbuidas de valores humanísticos, se rigen por métodos equiparables a los de las ciencias naturales \parencite[xxix]{potter1989}, sino de explotar las posibilidades que abre la disponibilidad de observar detalles en conjuntos masivos de textos literarios.
 
El obstáculo con que se encuentra esta nueva metodología reside en la limitación del conjunto de fenómenos textuales que pueden ser objeto de un análisis digital productivo, sea cuantitativo o cualitativo. La causa es que tales fenómenos requieren asociarse con características superficiales del texto que reconozca el computador, por lo general mediante la identificación de ciertos patrones. A pesar de que los computadores tienen una extraordinaria capacidad para ese tipo de análisis, su aporte se reduce a la resolución de problemas textuales operando al servicio de la intuición y perspicacia humana.

Las implicaciones metodológicas están claras, la mayoría de los estudios, tanto guiados como asistidos por datos, se ubican dentro del marco teórico de la estilística. Esto es, los textos serían constructos estéticos que causan una respuesta en el lector mediante el empleo de ciertos elementos en su estructura superficial. Estos llegan a ser tan sutiles que no se reconocen individualmente a simple vista, pero la suma de todos ellos determina la impresión global que provoca el texto. Sin embargo, el computador detecta estos microelementos e identifica patrones en su distribución, lo que puede ayudar a comprender cómo se alcanza dicho efecto estético \parencite[89-90]{rommel2004}.

Tales implicaciones metodológicas presentan una contrapartida para \citeauthor{rommel2004}~\parencite*[90-91]{rommel2004}, quien sostiene que esto lleva a simplificaciones injustas de lo que son los estudios basados en la computación, como que estos son poco más que «contar palabras»\footnote{Al respecto, Álvaro Cuéllar me llamó la atención sobre el prólogo del libro digital de José Manuel Fradejas \parencite*{fradejas2022}, un detallado manual para iniciar a los filólogos en el análisis estilístico con R\index{R} titulado \textit{Cuentapalabras}: el título es un dardo alusivo a que «los grandes de la materia filológica han desdeñado estos análisis como meros \textit{cuentapalabras} que no superan en nada las técnicas filológicas y de la hermenéutica, pues no tienen interés alguno en aquello que sea empírico y pueda refutar sus apreciaciones».}. Esto, a su vez, influencia de manera negativa las aproximaciones subsiguientes abordadas desde este punto de vista. Irónicamente, los estudios literarios que emplean herramientas digitales tienden a poner el acento en aspectos teóricos y metodológicos, tales como qué constituye un \textit{texto} y como la definición de tal concepto determina su análisis e interpretación. Resulta de crucial importancia la manera en que la crítica tradicional trata las características formales del texto y el papel de la electrónica en profundizar en ellas o abrir nuevas vías de entender el objeto de estudio.

A pesar de que aún quede mucho camino por recorrer, \citeauthor{rommel2004}~\parencite*[93]{rommel2004} considera que, desde al menos la década de 1970, las computadoras han proporcionado a la filología digital acceso prácticamente ilimitado a textos electrónicos de calidad, programas que dan la opción al filólogo de establecer los términos en que realiza la investigación —y no al revés— y equipos de gran capacidad de proceso y almacenamiento capaces de llevar a cabo las tareas propuestas.

Con todo, para que aplicar la computación a la filología sea factible, es conveniente ceñirse a una serie de buenas prácticas que garanticen tanto resultados productivos —ni triviales ni inválidos— como que los métodos y las herramientas no roben el foco al objeto de la investigación. \citeauthor{potter1991}~\parencite*[427-428]{potter1991} adelanta los siguientes ocho puntos para abordar los estudios literarios con una metodología digital.

\begin{enumerate}
	\item Considerar las particularidades lingüísticas del texto.
	\item Tener en cuenta la lógica del análisis del discurso.
	\item Basar el experimento en la teoría (y tener la preparación para poder criticarla si fuera necesario).
	\item Definir cómo la cuantificación pone a prueba la teoría.
	\item Evitar las computadoras hasta tener un plan de investigación definido.
	\item Diseñar una pequeña base de datos para poner a prueba puntos específicos.
	\item Reducir la fase computacional al mínimo.
	\item Describir el trabajo de una forma clara y replicable.
\end{enumerate}

\section{Una digresión epistemológica}
Resulta difícil hablar de filología digital sin referirse a los métodos cuantitativos. Sin ellos, la disciplina queda despojada de uno de sus principales valores. A cambio, debemos observancia a dos clases de principios. Por una parte, los ontológicos, que llevan a considerar los tipos de fenómenos textuales que se dan y las analogías que estos presentan según sus características contables. Por otra parte, hemos de prestar atención a los principios analíticos, que llevan a decidir qué conjuntos de propiedades son comparables, qué métodos se prestan para la comparación y cómo deben interpretarse los resultados. Incluso si nos limitamos a contar elementos textuales, es necesario definir el significado de \textit{contar} y de \textit{elementos textuales}: qué ha de contarse, por qué y cómo. Hay que definir también una topología con el corpus, las categorías en las que este se organiza y las reglas que las relacionan tanto con el corpus como entre sí. Finalmente, se necesita un marco analítico para explicar la manera en que las medidas de los datos textuales en una topología muestran diferencias entre sus categorías \parencite{gavin2022}. En este trabajo, nos concentramos en la ontología, dejando que nuestros resultados sean el objeto de la parte analítica.

Llegados a este punto, no se le escapará al lector que navegamos entre dos aguas, a un lado las del constructivismo y a otro las del positivismo. Por una parte, aquellos que consideran el texto literario como un constructo subjetivo, cada una de cuyas interpretaciones es tan válida como el lector o la escuela exegética estimen oportuno; por otro, los que sostienen que no existe interpretación aceptable más allá de la físicamente verificable. Nosotros, sin embargo, intentaremos abrirnos paso entre Escila y Caribdis.

\citeauthor{garcia2020}~\parencite*[15]{garcia2020} plantea las dificultades que encuentran los estudios literarios a resultas de no desarrollarse como una \textit{sucesión} a la manera que lo hacen las ciencias naturales. Pone como ejemplo la \textit{Poética} de Aristóteles, que aún hoy sigue siendo una lectura imprescindible. Más allá de su valor en la \textit{historia} de la estética teatral, es fundamental y válida como teoría \textit{viva}. Concluye que la poética está por ello más cercana a los clásicos literarios que a los científicos, en tanto que los últimos solo admiten la lectura como historia, pero no como ciencia. El estudio de la literatura, al igual que otras disciplinas humanísticas, no admite la simple reducción a un compendio de hechos objetivos. Coincidimos, pues, con \citeauthor{cameron2011} en su afirmación de que la mayoría de escuelas de crítica literaria 

\blockquote{\begin{english}do not seek to construct new empirically based knowledge, but are rather concerned with explication and interpretation – enterprises which have little to do with the teleological narrative of scientific `progress'. Granted, philologists might 'make progress' towards elucidating the meaning of ancient texts which were previously obscure because of gaps in our linguistic or historical knowledge, but critics do not `make progress' towards the `true' interpretation of a Keats sonnet; they merely offer different readings. Unlike a new scientific theorem, moreover, a new reading of a text does not automatically supersede all previous interpretations. Rather it is of interest to the extent that it reveals additional meanings in a text, proposes hitherto unnoticed connections between texts, or foregrounds themes in texts which resonate with the concerns of a particular moment.\end{english} \parencite[67]{cameron2011}}

Aquí, \citeauthor{cameron2011} responde a las tesis de \citeauthor{gottschall2008}~\parencite*{gottschall2010}, quien sostiene en la primera parte de su libro que la crítica literaria académica ha caído en la irrelevancia social; achaca esto a deficiencias metodológicas que impiden el progreso de la disciplina de una forma acumulativa como en las ciencias naturales. Critica asimismo la división entre las humanidades y otras disciplinas, ve en esta separación la causa que ha mantenido a la crítica literaria ajena a los métodos y teorías que han desarrollado otros campos. En su lugar, arguye \citeauthor{gottschall2008}, los estudios literarios se han enrocado en un castillo de naipes teórico, cuyas asunciones se sostienen mutuamente de manera circular, pero sin fundamento empírico: «it's turtles all the way down», podríamos resumirlo tomando prestada la conocida anécdota de Bertrand Russel \parencite{hawking1995}; en el mejor de los casos, matiza  \citeauthor{gottschall2008}, la circularidad acaba en el argumento de autoridad sentenciado en su momento por alguno de los grandes popes de la disciplina. A estas deficiencias, añade, hay que sumar una perversión en la actitud, pues considera que la misión de buena parte de las humanidades no es hacer avanzar el conocimiento, sino practicar alguna variante de activismo, por lo que su ejercicio académico es un mero medio para la consecución de objetivos ajenos a la búsqueda de la verdad.

Cabe criticarle a Gottschall que tome  tan solo una parte muy concreta de la academia anglosajona para representar un campo tan amplio como dispar. En concreto, dirige sus diatribas contra aquellos reductos en los que el \textit{constructivismo duro} es una ideología política que frisa la fe religiosa. Deja de lado las posiciones más moderadas, como las tradiciones críticas europeas, precisamente aquellas con los pies bien puestos en el suelo. El terreno de la ecdótica, por ejemplo, sigue relativamente libre de la dogmática posmoderna. Al crítico se le exige una mínima justificación textual para airear sus filias y fobias. No deja de asistirle la razón a \citeauthor{gottschall2008}, empero, en lo que respecta a la tendencia de las humanidades a elaborar marcos teóricos autorreferenciales blindados contra la falsación, y de ahí que convivan más o menos felizmente Aristóteles con el último grito en teorías críticas identitarias.

Parece sensata la afirmación de \citeauthor{cameron2011} de que no hay una \textit{verdadera} interpretación de Keats sino distintas lecturas. No obstante, habría que plantearse si todas ellas son igualmente válidas si admitimos que nuestro objeto de estudio es el texto —en su sentido más inclusivo, sin entrar en si desde el formalismo buscamos en él la literariedad o el estilo desde la estilística—. Pongamos por caso un ejemplo exagerado  —aunque nos tememos que perfectamente posible—: podría deconstruirse el poema «To the autumn» hasta concluir que se trata de una oda a la primavera. Por peregrina que pudiera resultar tal lectura a primera vista, poco se diferencia cualitativamente de interrogarlo \index{poema} para encontrar en él agravios sociales reales o imaginarios propios de nuestro tiempo. Para curarse en salud, podría incluso blindarse la conclusión con un marco teórico refractario a la refutación o, mejor aún, hacer del método una trampa saducea, de manera que cualquier intento de rebatir el argumento confirmara su validez.

No cabe duda de que las dos interpretaciones pueden ser una contribución a la encomiable causa de sendos exégetas hipotéticos. Por el contrario, en lo que respecta a la compresión de la obra literaria, no parece grande su aportación, si bien puede ser reveladora sobre el intérprete y su tiempo. Esto es, ninguna de las dos lecturas ficticias propuestas aspira a desentrañar la realidad estética del texto, sino imponerle los prejuicios personales del crítico: el texto no es un fin en sí mismo, es un medio\footnote{Resulta irónico que el tecnicismo usado sea precisamente \textit{interrogar} porque, en buena parte de los casos donde se usa, bien podría añadírsele «bajo tortura» sin forzar un ápice el significado, tal es la crudeza con la que se retuerce el sentido para, diríamos, extraer las confesiones.}.

Estas dos lecturas espurias, sin embargo, serían igual de válidas que una tercera que encuentre en los versos de Keats una evocación bucólica del otoño en el Hampshire rural basándose en la evidencia textual y biográfica. En este sentido, no podemos más que reconocer que \citeauthor{gottschall2008} pone el dedo en la llaga cuando señala que tanto la primacía del compromiso ideológico sobre el epistémico como la renuncia a la demostración empírica comprometen la credibilidad de las disciplinas humanísticas en su conjunto por acción u omisión. Tomamos prestadas las palabras de \citeauthor{garcia2020}~\parencite*[23]{garcia2020}: \blockquote{las ciencias humanas, en general, y la teoría literaria, en particular, deberían compartir con las ciencias propiamente dichas, por lo menos, los principios de racionalidad y honradez intelectual}.

Es indiscutible  que existen lecturas rigurosas desde el compromiso ideológico, en las que el texto tiene prevalencia sobre la imposición de la doctrina subyacente al método. Esto es, aquellas que interpretan la evidencia estética presente en la obra literaria a la luz de unas ideas, en lugar de embutir estas en la obra literaria para juzgarla, incluso a despecho de la evidencia. De esta manera, encontramos crítica literaria abiertamente política —valgan como ejemplo \citeauthor{benjamin1978} o \citeauthor{lukacs2009}— y, sin embargo, impecable si basa su razonamiento en los valores estéticos del texto, que interpreta observando unas premisas ideológicas particulares sin imponérselas.

La situación se complica cuando el marco teórico niega el conocimiento objetivo, como es el caso de aquellas escuelas que adoptan el escepticismo radical. Russell, con muy buen tino, comenta que este,
\blockquote{\begin{english}while logically impeccable, is psychologically impossible, and there is an element of frivolous insincerity in any philosophy which pretends to accept it. Moreover, if scepticism is to be theoretically defensible it must reject all inferences from what is experienced; a partial scepticism, such as the denial of physical events experienced by no one, or a solipsism which allows events in my future or in my unremembered past, has no logical justification, since it must admit principles of inference which lead to beliefs that it rejects.\end{english} \parencite[9]{russell1976}}

Huelga decir que la frívola falta de sinceridad que menciona el filósofo inglés es, precisamente, lo que lleva a las escuelas escépticas radicales a dudar de todos los presupuestos y marcos teóricos salvo uno en concreto: el suyo propio. En efecto, el escepticismo convenientemente selectivo de una escuela se traduce en un sistema autorreferencial, pues cuestiona todo fuera de él. Por lo tanto, es invulnerable a la refutación externa. En tanto que autojustificatorio, tampoco lo es desde dentro. El sistema es válido porque el sistema dice que es válido.

Por otro lado, tampoco faltan sistemas no necesariamente autorreferenciales que, sin embargo, plantean argumentos que no admiten refutación, por lo que caerían en otro tipo de trampa epistemológica. No obstante, su proceso de razonamiento resulta coherente dentro de su marco porque parten de evidencia para llegar a las conclusiones, válidas o no. \citeauthor{popper1989}~\parencite*[35]{popper1989} lo ejemplifica con dos casos, uno de un hombre que empuja a un niño al agua para ahogarlo y otro que sacrifica su vida intentando salvarlo. Ambos comportamientos tienen explicaciones análogas, tanto en términos freudianos como adlerianos. El primer caso sería para Freud un complejo de Edipo reprimido sublimado; el segundo, diría Adler, la necesidad de probar la valía por un complejo de inferioridad. Las observaciones de ambos psicoanalistas son correctas y sus explicaciones perfectamente plausibles dentro de la lógica interna de sendos sistemas.

Este tipo de razonamiento no acepta refutación porque ello exigiría una imposibilidad lógica: demostrar que los hombres de los ejemplos no padecían complejos. No abogamos con esto por descartar el psicoanálisis, como tampoco rechazamos métodos hermenéuticos por pecar, a su manera, de lo mismo que los dos psicoanalistas austriacos. Es innegable que las observaciones de Freud son agudas y certeras sin que la explicación que les atribuye a resultas de su marco teórico cambie eso una jota, y las plasma en una prosa sublime que, por sí misma, ya le habría ganado los laureles. Es más cuestionable empero la superioridad de un método hermenéutico especulativo como explicación última del texto.

Así pues, creemos que acierta \citeauthor{escobar2021}~\parencite*[35-40]{escobar2021} al optar por el realismo crítico como tercera vía. Arguye que la irreconciabilidad de ambas posturas solo tiene sentido en una epistemología de suma cero, esto es, si una forma de entender la realidad excluye a la segunda. Este no es el caso porque, como bien indica \citeauthor{cameron2011}, la literatura admite infinidad de lecturas que se suman una a otra y añaden matices adicionales a cómo se entiende el texto.

Los nuevos métodos cuantitativos no son una excepción. Una vez aceptado que en la obra de arte confluyen aspectos mensurables e inconmensurables, tiene poco sentido limitar las vías de estudio, renunciar a medir aquello que lo admite o despreciar cuanto sea incuantificable. Por el contrario, se obtiene una representación más compleja del objeto combinando ambas aproximaciones. No se trata de reemplazar un método de análisis literario por otro, sino de ir sumando nuevas perspectivas que proporcionen una imagen polifacética del texto, de combinar diferentes puntos de vista para ofrecer un reflejo más preciso y detallado, \blockquote{\begin{english}instead of saying that the humanities are besieged and giving up ground, we could truthfully say that these disciplines are discovering new missions and new ways to understand culture\end{english}} \parencite{underwood2019}.

Por lo tanto, adoptaremos una postura escéptica en el sentido popperiano \parencite{popper2002}, no hacia la ontología, sino hacia la epistemología. Aceptamos la existencia de una realidad externa objetiva, pero solo concedemos validez a la teoría sobre esta como una explicación incompleta y provisional. Incompleta, en tanto que se limita a describir aquellos aspectos que conocemos hasta el momento. Provisional, en cuanto que puede ser reemplazada si se encuentra una justificación más adecuada a los mismos fenómenos, como el modelo celeste copernicano sustituyó al ptolomeico.

No renunciaremos, empero, a la subjetividad, ya que, como dijimos, la excepcionalidad de la literatura —y, por extensión, del arte— radica en que buena parte del hecho artístico escapa a la cuantificación. Únicamente podemos aproximarnos a este lado inconmensurable acumulando apreciaciones subjetivas que muestren diferentes facetas del objeto de nuestra atención. Recordemos la historia de \textit{Rashomon}, en la que un bandido, la esposa de un samurái asesinado, un leñador y el espíritu del difunto ofrecen cuatro versiones contradictorias del homicidio. Desconocemos la realidad objetiva y no llegaremos nunca a conocerla. Son ejecuciones pictóricas de un mismo motivo desde ángulos variados, cada lienzo pintado con mayor o menor destreza, más preciso en unos detalles de la naturaleza que imita y menos a otros, unos con querencia hiperrealista y otros abstractos. Ninguna obra es completamente fiel al modelo y no hay dos obras iguales, ni siquiera aquellas compuestas sobre el mismo caballete. Sin embargo, la suma de todos ellos nos acerca al modelo real, incluso más allá su aspecto externo. En la película de Kuroshawa, este modelo natural son los hechos a esclarecer, en la crítica literaria, es la estética del texto. 

Nuestro propósito es, por lo tanto, facilitar la recopilación de pruebas forenses que complementen  las declaraciones de los testigos del crimen de \textit{Rashomon}, de dotar al delineante de papel milimetrado y plumilla para hacer una proyección diédrica ortogonal del motivo que inspira a los pintores. Se trata, en definitiva, de añadir otro modelo —imperfecto e incompleto, como el resto— que represente otros aspectos diferentes del texto literario.

\section{Lectura distante}\label{sec:disread}
Este trabajo pretende dotar a la filología digital de una herramienta de extracción de información que pueda ser usada en estudios tanto asistidos como guiados por datos. Por una parte, ha de facilitar las observaciones en el texto y sus metadatos según los criterios que el investigador estime relevantes y refinar sus pesquisas mediante la combinación de esos principios. Esto es, los datos deben servir como un archivo donde buscar y clasificar la información de la mejor manera posible. Por otra parte, los datos producidos han de poder tratarse digitalmente para realizar análisis digitales masivos y extraer las conclusiones de los resultados. Dicho de otro modo, debemos proporcionar el material con el que se lleven a cabo estudios guiados por datos. En el ámbito de la filología digital hay que hablar entonces de la \textit{lectura distante}\index{lectura distante}\footnote{\textit{Distant reading} en inglés.}.

El filólogo italiano Franco Moretti acuñó el término\index{lectura distante} como respuesta a la \textit{close reading}, como la de la escuela del Nuevo Criticismo. Arguye Moretti~\parencite*[57]{moretti2000} que esta última está limitada por la necesidad de circunscribirse a un corpus extremadamente reducido. Dedicar tanto tiempo a unos pocos textos implica la aceptación tácita de que solo un puñado de obras importa realmente. Añade que esta lectura tradicional es prácticamente un ejercicio teológico: el tratamiento solemne de unos pocos textos elegidos tomados muy seriamente. Contra esto, propone Moretti un «pacto con el diablo»: si sabemos cómo leer los textos, intentemos aprender cómo no hacerlo: la lectura distante,
\blockquote{\begin{english}where distant [...] \textit{is a condition of knowledge}: it allows you to focus on units that are much smaller or much larger than the text: devices, themes, tropes—or genres and systems. And if, between the very small and the very large, the text itself disappears, well, it is one of those cases when one can justifiably say less is more. If we want to understand the system in its entirety, we must accept losing something. We always pay a price for theoretical knowledge: reality is infinitely rich; concepts are abstract, are poor. But it's precisely this `poverty' that makes it possible to handle them, and therefore to know. This is why less is actually more.\end{english} \parencite[57-58]{moretti2000}}. En otras palabras, la diferencia conceptual entre la  lectura distante\index{lectura distante} y la lectura atenta no procede tanto de la extensión del corpus como de la escala del análisis y las observaciones que permiten hacerla. Para nuestro propósito, esto se traduce en el escrutinio de elementos mínimos, como el ritmo del verso, para encontrar patrones macroscópicos en sus combinaciones \parencite[p. 343, n. 2]{kroll2023}.

\citeauthor{moretti2017}~\parencite*{moretti2017} recurre a la máxima de Schleiermacher «entender al autor mejor que él mismo» para definir crítica textual, única justificación de la existencia de los críticos, y reconoce la genialidad del alemán por hallar el origen de tal  facultad en la ignorancia del crítico. Este, desconocedor del discurso que ha llevado al autor hasta el texto, se ve forzado a considerar una multitud de detalles de los que el autor no sería consciente. El obstáculo para la lectura distante\index{lectura distante} que ve Moretti en Schleiermacher es que, para el último, el objeto de la crítica es comprender el texto en su individualidad conforme a su intención, mientras que un corpus carece tanto de individualidad como de intención. 

Complementaremos a Moretti con un comentario de Friedrich Schlegel~\parencite*[p. 107; traducción propia]{schlegel1957} en términos sorprendentemente similares\footnote{
	No entraremos aquí en si la hermenéutica de Schleiermacher bebió del filólogo, pero animamos al lector a profundizar en un interesantísimo caso de transtextualidad. Ver, por ejemplo \cite[p. 436 nn. 13-15]{patsch1966}.
}: «La crítica no es realmente otra cosa que la comparación del espíritu y las letras de una obra, lo cual se trata como infinito, como absoluto e individuo. Crítica significa comprender un autor mejor de lo que él mismo se ha comprendido»\footnote{
	«\begin{textgerman}Kritik ist eigentlich nichts als Vergleichung des Geistes und des Buchstabens eines Werks, welches als \textit{Unendliches}, als Absolutum und Individuum behandelt wird – \textit{Kritisieren} heißt einen Autor besser verstehn als er sich selbst verstanden hat\end{textgerman}».
}.

Además de lo postulado por Schleiermacher, Schlegel aporta un nuevo factor que resulta muy conveniente para nuestra empresa, a saber: la crítica como comparación entre la forma externa y el significado profundo del texto. Nos tomaremos aquí la licencia de forzar el sentido de manera anacrónica\footnote{Anacrónica solo en cierto modo. No olvidemos que la exégesis numerológica es anterior a Schlegel, como, por ejemplo, la practicada por ciertas escuelas cabalísticas.} para llevárnoslo a nuestro terreno; adoptaremos en su literalidad la afirmación  del filósofo. No hace falta decir que, de esta manera, nos encontramos precisamente con el objeto de los estudios de lectura distante\index{lectura distante}.

Si retomamos la conocida analogía del pudin con la que  \citeauthor{wimsatt1954}~\parencite*[4-5]{wimsatt1954} ilustraban la falacia intencional, los métodos digitales no proporcionan una crítica gastronómica del plato, sino la receta o, mejor dicho, las recetas exactas de cada uno de las comidas que se han servido en un restaurante. Hallamos así una fórmula para expresar la distinción entre el pudin de un determinado local y los equivalentes de la competencia. Evidentemente, esto no proporcionará la experiencia sensual que se obtiene mediante la degustación, pero sí puede contribuir a explicar las sensaciones placenteras que se experimentan al saborearlo. Este método, claro está, solo reconoce los ingredientes que se encuentran en el plato; nada dice de los sentimientos del cocinero o cómo quería este que supiera su creación; afortunadamente, todos juzgamos un pudin —al menos en privado— sin otro criterio que nuestro paladar. Lo que tampoco tendremos es una descripción precisa que permita reproducir el toque especial de los grandes chefs, esa combinación de saber apreciar los detalles y obrar en consecuencia, la variable oculta que rompe el determinismo, aquello que marca la diferencia entre el prosaico condumio y la obra de arte gastronómico, entre el escritor y el literato.

La crítica digital, apunta Moretti~\parencite*[6-7]{moretti2017}, está reemplazando significados con patrones, pero también interpreta estos a partir de palabras. La ausencia de contradicción radica en que el término \textit{significado}\footnote{En el original \textit{meaning}.} se interpreta de modos diferentes, y Moretti lo explica valiéndose de los términos alemanes empleados por Schleiermacher \textit{sentido} y \textit{significado}\footnote{En el alemán original de Schleiermacher \textit{Sinn} y \textit{Bedeutung}, respectivamente. Moretti los traduce al italiano como \textit{senso} y \textit{significato} y al inglés como \textit{significance} y \textit{meaning}.}. Al final, se trata del significado del elemento en contexto o aislado. Ahí es donde entra en escena la interpretación: encontrar el sentido del texto más allá del significado de sus componentes individuales. Para ello, es imprescindible considerar también aquello que el autor conoce y no explicita.

Moretti~\parencite*[67]{moretti2003} ilustra la lectura distante\index{lectura distante} por analogía con la historiografía antigua, que solo tenía ojos para los eventos extraordinarios y desdeñaba lo consuetudinario. El cambio de paradigma que propició la Escuela de los Annales llevó a considerar la \textit{gran masa de hechos}. Se trata, pues, de poner el foco literario sobre esta y no en la singularidad de las grandes obras canónicas. Pone como ejemplo la literatura británica del siglo \textsc{xix}, de la cual, un mínimo corpus de apenas unas doscientas novelas sería ya generoso, incluso notablemente mayor de lo que hoy podríamos considerar como el canon de la época. A pesar de todo, esto no representa ni una centésima parte de las obras que llegaron a publicarse en aquel periodo. Para leerlas todas, necesitaríamos leer una novela cada día durante más de un siglo. Sin embargo, pretendemos entender toda esta producción con un puñado de casos individuales que, para más inri, han entrado en el canon precisamente por su excepcionalidad. Concluye diciendo que no estamos hablando de la suma de casos individuales sino de un complejo sistema colectivo que debe intentarse aprehender como tal.

En definitiva, \blockquote{\begin{english}you reduce the text to a few elements, and abstract them, and construct a new, artificial object. A model. And at this point you start working at a ‘secondary’ level, removed from the text: a map, after all, is always a look from afar—or is useless, like Borges’s map of the empire. Distant reading, I have called this work elsewhere; where distance is however not an obstacle, but a specific form of knowledge: fewer elements, hence a sharper sense of their overall interconnection. Shapes, relations, structures. Patterns.\end{english} \parencite[94]{moretti2004}}

La lectura distante\index{lectura distante} sigue dependiendo de la atenta para obtener esos pocos elementos necesarios para construir el modelo, si bien esta se delega. La lectura distante por sí misma ofrece una solución teórica a un problema que, a efectos prácticos, solo sirve para terminar trasladándolo. Pensemos en la literatura como un vasto campo en el que el canon constituye una porción mínima del total, insuficiente para describirlo; explicar el hecho literario mediante su canon sería como tratar de reducir una extensa pradera de pastos a unas pocas flores de brillantes colores que se destacan entre la hierba. La lectura distante propone tomar elementos mínimos de las caracterizaciones hechas por terceros: de las flores vistosas, pero también de las menos; plantas, hierbas, piedras, charcos... incluso —si se me permite— de evidencias \textit{post-hoc} de presencia bovina. Con todo, esto no elimina el sesgo de la crítica literaria. Buscaríamos los componentes mínimos de lo que hay en el prado de nuestra analogía en el catálogo de una floristería; esto es, nos limitaríamos a unas pocas flores, ignorando las menos vistosas así como todo lo demás, lo que Margaret Cohen \parencite*[23]{cohen1999} denominó «the great unread». Resultaría poco factible repartir el trabajo de forma satisfactoria: no solo requeriría coordinar un exorbitante número de personas, sino que, además, el reparto jamás sería justo, en tanto que apenas hay una reducida cantidad de obras maestras en comparación con la inmensidad de otras, no solo menores y mediocres, sino también insufribles. 

Debemos, pues, allanar el camino a la lectura distante\index{lectura distante}, de manera que, aprovechando que nos concentramos en aspectos simples concretos, sea la máquina la que se encargue de escudriñar los textos en pos de ellos. Evitemos llevarnos a engaño: la automatización no elimina por completo la necesidad de cierta intervención manual en el análisis porque los textos han de digitalizarse en primer lugar y prepararse para que el computador sea capaz de interpretarlos; no obstante, incluso teniendo esto en cuenta, los recursos temporales y humanos se reducirían varios órdenes de magnitud. Lo que nos proponemos no es otra cosa que encargar al computador la búsqueda de los elementos mínimos en piezas teatrales áureas, como Moretti \parencite*{moretti2000a} puso a sus alumnos a hacer lo propio en novelas policíacas. Claro está que, además, lo hacemos con la intención de multiplicar el número de obras analizadas mientras reducimos el tiempo a una fracción; de lo contrario, nuestro propósito práctico carecería de sentido.
